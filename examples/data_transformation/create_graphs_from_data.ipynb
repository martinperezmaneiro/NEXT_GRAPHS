{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7a41f48",
   "metadata": {},
   "source": [
    "## NB of the functions (construction and test) to transform the MC labelled events into graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3030f946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import tables as tb\n",
    "import networkx as nx\n",
    "import os.path as osp\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import invisible_cities.io.dst_io as dio\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data.makedirs import makedirs\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import BatchNorm1d, CrossEntropyLoss\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66736991",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "file_nexus = '/mnt/lustre/scratch/nlsas/home/usc/ie/mpm/NEXT100/data/pressure_topology/13bar/0nubb/label/prod/nexus_label_{n}_0nubb.h5'\n",
    "file_graph = '/mnt/lustre/scratch/nlsas/home/usc/ie/mpm/NEXT100/data/pressure_topology/13bar/0nubb/graph_nn/prod/nexus_graph_nn_{n}_0nubb.pt'\n",
    "\n",
    "df = dio.load_dst(file_nexus.format(n=i), 'DATASET', 'MCVoxels')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6bae1baf",
   "metadata": {},
   "source": [
    "Function to create graphs with Networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "84bcabce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(df, max_distance, coords):\n",
    "    '''\n",
    "    Takes a dataframe and creates a graph with the coordinates as nodes, which are connected by edges \n",
    "    if they are separated less than certain distance.\n",
    "    \n",
    "    Args:\n",
    "        df: DATAFRAME\n",
    "    Contains spatial information (at least).\n",
    "        \n",
    "        max_distance: FLOAT\n",
    "    Indicates the maximum distance between nodes to be connected.\n",
    "        \n",
    "        coords: LIST OF STR\n",
    "    Indicates the names of the df columns that have the coordinates info.\n",
    "    \n",
    "    RETURNS: \n",
    "        graph: NETWORKX GRAPH\n",
    "    Graph with the nodes and their connections.\n",
    "    '''\n",
    "    \n",
    "    nodes = [tuple(x) for x in df[coords].to_numpy()]\n",
    "    \n",
    "    graph = nx.Graph()\n",
    "    graph.add_nodes_from(nodes)\n",
    "    \n",
    "    #Ahora hacemos los edges para contar las componentes conexas\n",
    "    for va, vb in itertools.combinations(graph.nodes(), 2):\n",
    "        va_arr, vb_arr = np.array(va), np.array(vb)\n",
    "        dis = np.linalg.norm(va_arr-vb_arr)\n",
    "        if dis <= max_distance:\n",
    "            graph.add_edge(va, vb, distance = dis)\n",
    "    return graph"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b5bbe695",
   "metadata": {},
   "source": [
    "Functions to transform our data into data for a graph neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19047d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_index(event, \n",
    "               max_distance = np.sqrt(3), \n",
    "               norm_features = True,\n",
    "               ener_name = 'ener', \n",
    "               coord_names = ['xbin', 'ybin', 'zbin'], \n",
    "               directed = False, \n",
    "               fully_connected = False):\n",
    "    ''' \n",
    "    Creates the edge index tensor, with shape [2, E] where E is the number of edges.\n",
    "    It contains the index of the nodes that are connected by an edge. \n",
    "    Also creates the edge features tensor, with shape [E, D] being D the number of features. In this case we add the distance, and a sort of gradient.\n",
    "    Also creates the edge weights tensor, with shape E: one weight assigned to each edge. In this case we use the inverse of the distance. \n",
    "    '''\n",
    "    def grad(ener, dis, i, j): return abs(ener[i] - ener[j]) / dis\n",
    "    def inve(dis): return 1 / dis\n",
    "\n",
    "    coord = event[coord_names].T\n",
    "    ener  = event[ener_name]\n",
    "    ener = ener / sum(ener) if norm_features else ener\n",
    "    edges, edge_features, edge_weights = [], [], []\n",
    "    node_comb = itertools.combinations if directed else itertools.permutations\n",
    "    for i, j in node_comb(coord, 2):\n",
    "        dis = np.linalg.norm(coord[i].values - coord[j].values)\n",
    "        #append info for all edges if fully_connected, or if not, only the edges for the closest nodes\n",
    "        if fully_connected or dis <= max_distance:\n",
    "            edges.append([i, j])\n",
    "            edge_features.append([dis, grad(ener, dis, i, j)])\n",
    "            edge_weights.append(inve(dis))\n",
    "    edges, edge_features, edge_weights = torch.tensor(edges, dtype = torch.long).T, torch.tensor(edge_features), torch.tensor(edge_weights)\n",
    "    return edges, edge_features, edge_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d04cb38a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 12210])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index(df[df.dataset_id == 10], coord_names = ['x', 'y', 'z'], directed = False, fully_connected=True)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ad818ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphData(event, \n",
    "              data_id, \n",
    "              features = ['energy'], \n",
    "              label_n = ['segclass'], \n",
    "              norm_features = True, \n",
    "              max_distance = np.sqrt(3), \n",
    "              ener_name = 'energy', \n",
    "              coord_names = ['xbin', 'ybin', 'zbin'], \n",
    "              directed = False, \n",
    "              fully_connected = False, \n",
    "              simplify_segclass = False):\n",
    "    event.reset_index(drop = True, inplace = True)\n",
    "    edges, edge_features, edge_weights = edge_index(event, \n",
    "                                                    max_distance=max_distance, \n",
    "                                                    norm_features = norm_features,\n",
    "                                                    ener_name=ener_name, \n",
    "                                                    coord_names=coord_names, \n",
    "                                                    directed=directed, \n",
    "                                                    fully_connected=fully_connected)\n",
    "    #nodes features, for now just the energy; the node itself is defined by its position\n",
    "    features = event[features]\n",
    "    features = features / features.sum() if norm_features else features\n",
    "    nodes = torch.tensor(features.values)\n",
    "    #nodes segmentation label\n",
    "    seg = event[label_n].values\n",
    "    if simplify_segclass:\n",
    "        label_map = {1:1, 2:2, 3:3, 4:1, 5:2, 6:3, 7:4}\n",
    "        seg = np.array([label_map[i] for i in seg])\n",
    "    #we can try to add also the transformation just to have track + blob (+ ghost)\n",
    "    #shifting already the label below!!\n",
    "    label = torch.tensor(seg - 1)\n",
    "    coords = torch.tensor(event[coord_names].values)\n",
    "    bincl = event.binclass.unique()[0]\n",
    "    graph_data = Data(x = nodes, edge_index = edges, edge_attr = edge_features, edge_weight = edge_weights, y = label, num_nodes = len(nodes), coords = coords, dataset_id = data_id, binclass = bincl)\n",
    "    return graph_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "74d9366b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[111, 2], edge_index=[2, 12210], edge_attr=[12210, 2], y=[111, 1], edge_weight=[12210], num_nodes=111, coords=[111, 3], dataset_id=10, binclass=1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_obj = graphData(df[df.dataset_id == 10], 10, features = ['ener', 'nhits'], norm_features = True, ener_name = 'ener', coord_names = ['x', 'y', 'z'], directed = False, fully_connected=True)\n",
    "\n",
    "data_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9e7ab8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphDataset(file, \n",
    "                 group = 'DATASET', \n",
    "                 table = 'BeershebaVoxels', \n",
    "                 id = 'dataset_id', \n",
    "                 features = ['energy'], \n",
    "                 label_n = ['segclass'], \n",
    "                 norm_features = True, \n",
    "                 ener_name = 'energy', \n",
    "                 max_distance = np.sqrt(3), \n",
    "                 coord_names = ['xbin', 'ybin', 'zbin'], \n",
    "                 directed = False, \n",
    "                 fully_connected = False, \n",
    "                 simplify_segclass = False):\n",
    "    df = dio.load_dst(file, group, table)\n",
    "    dataset = []\n",
    "    for dat_id, event in df.groupby(id):\n",
    "        #event = event.reset_index(drop = True) #esto lo hace ahora dentro de graphData\n",
    "        graph_data = graphData(event, \n",
    "                               dat_id, \n",
    "                               features=features, \n",
    "                               label_n=label_n, \n",
    "                               norm_features = norm_features, \n",
    "                               max_distance=max_distance, \n",
    "                               ener_name = ener_name, \n",
    "                               fully_connected = fully_connected, \n",
    "                               coord_names=coord_names, \n",
    "                               directed = directed, \n",
    "                               simplify_segclass = simplify_segclass)\n",
    "        #to avoid events with no graph connections, they can be discarted\n",
    "        if graph_data.edge_index.numel() == 0:\n",
    "            continue\n",
    "        dataset.append(graph_data)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "41b8271d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '/mnt/lustre/scratch/nlsas/home/usc/ie/mpm/NEXT100/data/pressure_topology/13bar/0nubb/label/prod/nexus_label_{}_0nubb.h5'\n",
    "#dataset_1 = graphDataset(file.format('1'), table = 'MCVoxels', features = ['ener', 'nhits'], ener_name = 'ener', coord_names = ['x', 'y', 'z'], directed = False, fully_connected = True)\n",
    "#dataset_2 = graphDataset(file.format('2'))\n",
    "#dataset_3 = graphDataset(file.format('3'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "794f2b99",
   "metadata": {},
   "source": [
    "We can read the stored tensors and then group them into batches. The batch joins all the nodes, edge_index, label, etc... and will have a batch list of the length of the nodes, with corresponding indexes to recognize the different graphs of course. In the case of the edges, they will continue to have two lists of positions for the nodes that have an edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "513912ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyDataset(Dataset):\n",
    "#     def __init__(self, root, tag = '0nubb', transform=None, pre_transform=None, pre_filter=None, directed = False, simplify_segclass = False):\n",
    "#         self.sort = lambda x: int(x.split('_')[-2])\n",
    "#         self.tag = tag\n",
    "#         self.directed = directed\n",
    "#         self.simplify_segclass = simplify_segclass\n",
    "#         super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        \n",
    "#     @property\n",
    "#     def raw_file_names(self):\n",
    "#         ''' \n",
    "#         Returns a list of the raw files in order (supossing they are beersheba labelled files that have the structure beersheba_label_N_tag.h5)\n",
    "#         '''\n",
    "#         rfiles = [i.split('/')[-1] for i in glob(self.raw_dir + '/*_{}.h5'.format(self.tag))]\n",
    "#         return sorted(rfiles, key = self.sort)\n",
    "\n",
    "#     @property\n",
    "#     def processed_file_names(self):\n",
    "#         '''\n",
    "#         Returns a list of the processed files in order (supossing they are stored tensors with the structure data_N.pt)\n",
    "#         '''\n",
    "#         pfiles = [i.split('/')[-1] for i in glob(self.processed_dir + '/data_*_{}.pt'.format(self.tag))]\n",
    "#         return sorted(pfiles, key = self.sort)\n",
    "    \n",
    "#     def process(self):\n",
    "#         makedirs(self.processed_dir)\n",
    "#         already_processed = [self.sort(i) for i in self.processed_file_names]\n",
    "#         for raw_path in self.raw_paths:\n",
    "#             idx = self.sort(raw_path)\n",
    "#             if np.isin(idx, already_processed):\n",
    "#                 #to avoid processing already processed files\n",
    "#                 continue\n",
    "#             data = graphDataset(raw_path, directed=self.directed, simplify_segclass=self.simplify_segclass)\n",
    "\n",
    "#             torch.save(data, osp.join(self.processed_dir, f'data_{idx}_{self.tag}.pt'))\n",
    "        \n",
    "\n",
    "#     def len(self):\n",
    "#         return len(self.processed_file_names)\n",
    "\n",
    "#     def get(self, idx):\n",
    "#         data = torch.load(osp.join(self.processed_dir, f'data_{idx}_{self.tag}.pt'))\n",
    "#         return data\n",
    "\n",
    "#     def join(self):\n",
    "#         #print('Joining ', self.processed_file_names)\n",
    "#         dataset = []\n",
    "#         for processed_path in self.processed_paths:\n",
    "#             dataset += torch.load(processed_path)\n",
    "#         return dataset\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78b1c542",
   "metadata": {},
   "source": [
    "Here we create a dataset file(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c348f06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = '/mnt/lustre/scratch/nlsas/home/usc/ie/mpm/NEXT100/labelled_data/0nubb/554mm_voxels/'\n",
    "# tag = file_path.split('/')[-3]\n",
    "# Dataset(file_path, '0nubb', directed = False).process() #this action creates the processed files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0da386",
   "metadata": {},
   "source": [
    "## Test creating and saving files with graphData "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8435fdfd",
   "metadata": {},
   "source": [
    "We want to detect where the float64 change is done, instead of maintaining the tensors always as float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6be2cc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gzip\n",
    "import torch\n",
    "import numpy as np\n",
    "import itertools\n",
    "from   torch_geometric.data import Data\n",
    "import invisible_cities.io.dst_io as dio\n",
    "sys.path.append(\"/home/usc/ie/mpm/NEXT_graphs/\") # go to parent dir\n",
    "from NEXT_graphNN.utils.data_loader import graphDataset, graphData, edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a34ad30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = '1eroi'\n",
    "file_in = '/home/usc/ie/mpm/prueba/nexus_label_1_{}.h5'.format(dt)\n",
    "fileout = '/home/usc/ie/mpm/prueba/nexus_gnn_1_{}_float32.pt'.format(dt)\n",
    "\n",
    "group = 'DATASET'\n",
    "table = 'MCVoxels'\n",
    "\n",
    "id_name     = 'dataset_id'\n",
    "ener_name   = 'ener'\n",
    "features    = ['ener', 'nhits']\n",
    "label_n     = ['segclass']\n",
    "coord_names = ['x', 'y', 'z']\n",
    "\n",
    "norm_features = True\n",
    "max_distance  = np.sqrt(3)\n",
    "\n",
    "directed          = False\n",
    "fully_connected   = True\n",
    "simplify_segclass = False\n",
    "\n",
    "get_file_number = lambda filename: int(filename.split(\"/\")[-1].split(\"_\")[-2])\n",
    "\n",
    "dat_id = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc512483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = graphDataset(file_in, \n",
    "#                            group             = group, \n",
    "#                            table             = table,\n",
    "#                            id_name           = id_name, \n",
    "#                            feature_n         = features, \n",
    "#                            label_n           = label_n, \n",
    "#                            norm_features     = norm_features,\n",
    "#                            max_distance      = max_distance, \n",
    "#                            ener_name         = ener_name,\n",
    "#                            coord_names       = coord_names, \n",
    "#                            directed          = directed, \n",
    "#                            fully_connected   = fully_connected, \n",
    "#                            simplify_segclass = simplify_segclass,\n",
    "#                            get_fnum_function = get_file_number, \n",
    "#                            torch_dtype       = torch.float)\n",
    "\n",
    "\n",
    "# with gzip.open(fileout + '.gz', 'wb') as fout:\n",
    "#     torch.save(dataset, fout)\n",
    "# torch.save(dataset, fileout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d8bdcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c493be9",
   "metadata": {},
   "source": [
    "## Create dataset from several processed files\n",
    "\n",
    "With the graphDataset function I've processed each individual file. Now the goal is to read all the files I want to create a complete dataset, shuffle them and divide between test, train and validation batches, and saving them again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8704a803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "def load_graph_data(fname):\n",
    "    if isinstance(fname, list):\n",
    "        dataset = [graph for path in fname for graph in torch.load(path)]\n",
    "    if isinstance(fname, str):\n",
    "        dataset = torch.load(fname)\n",
    "    return dataset\n",
    "\n",
    "def load_graph_data_compressed(fname):\n",
    "    dataset = []\n",
    "    if isinstance(fname, list):\n",
    "        for path in fname:\n",
    "            print(path)\n",
    "            with gzip.open(path, 'rb') as f:\n",
    "                dataset.extend(torch.load(f))\n",
    "    if isinstance(fname, str):\n",
    "        with gzip.open(fname, 'rb') as f:\n",
    "            dataset.extend(torch.load(f))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da863ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = '/mnt/lustre/scratch/nlsas/home/usc/ie/mpm/NEXT100/data/pressure_topology/{p}/{dt}/graph_nn/prod/{f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "747cb8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = '13bar'\n",
    "#dt = '*'\n",
    "f = '*.pt.gz'\n",
    "#numero aprox de eventos por fichero\n",
    "nevents_per_file_0nubb = 400\n",
    "nevents_per_file_1eroi = 50\n",
    "\n",
    "#numero de eventos que queremos por data type\n",
    "wanted_nevents_per_dt = 2000\n",
    "#numero de ficheros necesarios para cada dt para tener el numero de eventos anterior\n",
    "nfiles_0nubb = int(wanted_nevents_per_dt / nevents_per_file_0nubb)\n",
    "nfiles_1eroi = int(wanted_nevents_per_dt / nevents_per_file_1eroi)\n",
    "\n",
    "#ficheros seleccionados por cada dt\n",
    "files_0nubb = sorted(glob(basedir.format(p = p, dt = '0nubb', f = f)), key = lambda x: (x.split('/')[-4], int(x.split('_')[-2])))[:nfiles_0nubb]\n",
    "files_1eroi = sorted(glob(basedir.format(p = p, dt = '1eroi', f = f)), key = lambda x: (x.split('/')[-4], int(x.split('_')[-2])))[:nfiles_1eroi]\n",
    "\n",
    "#ficheros totales, con 2 veces el numero de eventos aprox por dt\n",
    "files = files_0nubb + files_1eroi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58002c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/mnt/lustre/scratch/nlsas/home/usc/ie/mpm/NEXT100/data/pressure_topology/13bar/0nubb/graph_nn/prod/nexus_graph_nn_1_0nubb.pt.gz',\n",
       " '/mnt/lustre/scratch/nlsas/home/usc/ie/mpm/NEXT100/data/pressure_topology/13bar/0nubb/graph_nn/prod/nexus_graph_nn_2_0nubb.pt.gz',\n",
       " '/mnt/lustre/scratch/nlsas/home/usc/ie/mpm/NEXT100/data/pressure_topology/13bar/0nubb/graph_nn/prod/nexus_graph_nn_3_0nubb.pt.gz',\n",
       " '/mnt/lustre/scratch/nlsas/home/usc/ie/mpm/NEXT100/data/pressure_topology/13bar/0nubb/graph_nn/prod/nexus_graph_nn_4_0nubb.pt.gz',\n",
       " '/mnt/lustre/scratch/nlsas/home/usc/ie/mpm/NEXT100/data/pressure_topology/13bar/0nubb/graph_nn/prod/nexus_graph_nn_5_0nubb.pt.gz']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f49cdab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "fileout = '/mnt/lustre/scratch/nlsas/home/usc/ie/mpm/NEXT100/data/pressure_topology/{p}/dataset_{p}_graph_nn_4kevents.pt.gz'\n",
    "# dataset = load_graph_data_compressed(files)\n",
    "# dataset_len = len(dataset)\n",
    "\n",
    "# # Define the sizes for training, validation, and test sets\n",
    "# train_size = int(0.8 * dataset_len)\n",
    "# val_size = int(0.1 * dataset_len)\n",
    "# test_size = dataset_len - train_size - val_size\n",
    "\n",
    "# assert train_size + val_size + test_size == dataset_len\n",
    "# # Split the dataset\n",
    "# split_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "\n",
    "# # with gzip.open(fileout.format(p = p), 'wb') as fout:\n",
    "# #     torch.save(split_dataset, fout)\n",
    "\n",
    "# torch.save(split_dataset, fileout.format(p = p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4545d554",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.load(fileout.format(p = p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e8f1f1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = dataset\n",
    "batch_size = 10\n",
    "# Shuffle and create DataLoader for each set\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5af8691",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb54bf68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[1526, 2], edge_index=[2, 233728], edge_attr=[233728, 2], y=[1526, 1], edge_weight=[233728], num_nodes=1526, coords=[1526, 3], dataset_id=[10], binclass=[10], fnum=[10], batch=[1526], ptr=[11])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "00b01a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(163)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(batch.batch == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fa52460b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 1, 0, 0, 0, 1, 1]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.binclass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95cabf1d",
   "metadata": {},
   "source": [
    "## Creating dataset from mixed hdf file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3288cd92",
   "metadata": {},
   "source": [
    "Need this bc we now need to do the loop and divide per event (as I have a big file) instead of a loop on lots of small files..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3ea86d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_voxel_mix = '/mnt/lustre/scratch/nlsas/home/usc/ie/mpm/NEXT100/labelled_data/train_data_files/mixer_voxels_fid_norm.h5'\n",
    "process_file_voxel_mix = '/mnt/lustre/scratch/nlsas/home/usc/ie/mpm/NEXT100/labelled_data/train_data_files/process_mixer_voxels_fid_norm/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e11b73f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphProcess(file, \n",
    "                 outfile,\n",
    "                 nevents_per_file = 200,\n",
    "                 table = 'MCVoxels', \n",
    "                 id = 'dataset_id', \n",
    "                 features = ['ener'], \n",
    "                 label_n = ['segclass'], \n",
    "                 max_distance = np.sqrt(3), \n",
    "                 coord_names = ['x', 'y', 'z'], \n",
    "                 directed = False, \n",
    "                 simplify_segclass = False):\n",
    "    df = pd.read_hdf(file, table)\n",
    "    dataset = []\n",
    "    for dat_id, event in df.groupby(id):        \n",
    "        # event = event.reset_index(drop = True)\n",
    "        graph_data = graphData(event, dat_id, features=features, label_n=label_n, max_distance=max_distance, coord_names=coord_names, directed = directed, simplify_segclass = simplify_segclass)\n",
    "        dataset.append(graph_data)\n",
    "        \n",
    "        if (dat_id + 1) % nevents_per_file == 0:\n",
    "            start_id = (dat_id + 1) - nevents_per_file\n",
    "            final_id = dat_id\n",
    "            torch.save(dataset, osp.join(outfile, f'data_{start_id}_{final_id}.pt'))\n",
    "            dataset = []\n",
    "\n",
    "    #to save the last file!!\n",
    "    start_id = (dat_id + 1) - nevents_per_file\n",
    "    final_id = dat_id\n",
    "    torch.save(dataset, osp.join(outfile, f'data_{start_id}_{final_id}.pt'))\n",
    "    \n",
    "    #return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fadeda02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#graphProcess(file_voxel_mix, process_file_voxel_mix, nevents_per_file=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "568fdbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted(glob(process_file_voxel_mix + 'data_*'), key = lambda x: int(x.split('_')[-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d88c8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_graphs = load_graph_data(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e3b54f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(all_graphs, osp.join(process_file_voxel_mix, f'all_proc_mix_vox_fid_norm.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e6512580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from invisible_cities.types.ic_types import AutoNameEnumBase\n",
    "from enum import auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "25a5c491",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelType(AutoNameEnumBase):\n",
    "    Classification = auto()\n",
    "    Segmentation   = auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "fb501c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_loss(fname, label_type, nclass = 3, nevents = None):\n",
    "    dataset = load_graph_data(fname)[:nevents]\n",
    "    if label_type==LabelType.Segmentation:\n",
    "        inv_freq = 1 / sum([np.bincount(graph.y.numpy().flatten(), minlength=nclass) for graph in dataset])\n",
    "    elif label_type == LabelType.Classification:\n",
    "        inv_freq = 1 / np.bincount([graph.binclass for graph in dataset])\n",
    "    return inv_freq / sum(inv_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3e3ff230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.36001924, 0.07829171, 0.56168905])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_loss(files, LabelType.Segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c4f6c458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.26362139, 0.73637861])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_loss(files, LabelType.Classification)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4ecbda2",
   "metadata": {},
   "source": [
    "PARA MI DATASET ACTUAL LOS LOSSWEIGHT DE SEGMENTACION SON array([0.36001924, 0.07829171, 0.56168905])\n",
    "\n",
    "Y LOS DE CLASIFICACION SON array([0.26362139, 0.73637861])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a0e62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b778ca0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_idx_split(dataset, train_perc):\n",
    "    '''\n",
    "    Divides the whole dataset into train, validation and test data. Picks a certain percentage (the majority) for the \n",
    "    train batch, and the remaining is divided equally for validation and test.\n",
    "    '''\n",
    "    indices = np.arange(len(dataset))\n",
    "    valid_perc = (1 - train_perc) / 2\n",
    "    random.shuffle(indices)\n",
    "    train_data = torch.tensor(np.sort(indices[:int((len(indices)+1)*train_perc)])) \n",
    "    valid_data = torch.tensor(np.sort(indices[int((len(indices)+1)*train_perc):int((len(indices)+1)*(train_perc + valid_perc))]))\n",
    "    test_data = torch.tensor(np.sort(indices[int((len(indices)+1)*(train_perc + valid_perc)):]))\n",
    "    idx_split = {'train':train_data, 'valid':valid_data, 'test':test_data}\n",
    "    return idx_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1769eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, train_perc):\n",
    "    '''\n",
    "    Divides the whole dataset into train, validation and test data. Picks a certain percentage (the majority) for the \n",
    "    train batch, and the remaining is divided equally for validation and test.\n",
    "    '''\n",
    "\n",
    "    valid_perc = (1 - train_perc) / 2\n",
    "    nevents = len(dataset)\n",
    "    train_data = dataset[:int(nevents * train_perc)]\n",
    "    valid_data = dataset[int(nevents * train_perc):int(nevents * (train_perc + valid_perc))]\n",
    "    test_data  = dataset[int(nevents * (train_perc + valid_perc)):]\n",
    "    return train_data, valid_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a360843",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr, va, te = split_dataset(all_graphs, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "15636af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(tr, osp.join(process_file_voxel_mix, f'train_graphs.pt'))\n",
    "# torch.save(va, osp.join(process_file_voxel_mix, f'valid_graphs.pt'))\n",
    "# torch.save(te, osp.join(process_file_voxel_mix, f'test_graphs.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "df46c1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mcvox = pd.read_hdf(file_voxel_mix, 'MCVoxels')\n",
    "# evinf = pd.read_hdf(file_voxel_mix, 'EventsInfo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "47a349b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datapath = '/mnt/lustre/scratch/nlsas/home/usc/ie/mpm/NEXT100/labelled_data/train_data_files/'\n",
    "\n",
    "# ids = [g.dataset_id for g in te]\n",
    "# selected_mcvox = mcvox[np.isin(mcvox.dataset_id, ids)]\n",
    "# selected_evinf = evinf[np.isin(evinf.dataset_id, ids)]\n",
    "\n",
    "# selected_mcvox.to_hdf(datapath + 'test_data.h5', 'MCVoxels')\n",
    "# selected_evinf.to_hdf(datapath + 'test_data.h5', 'EventsInfo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "4c5f92f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818a71d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48796f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(*,\n",
    "              nepoch,\n",
    "              train_data,\n",
    "              valid_data,\n",
    "              train_batch_size,\n",
    "              valid_batch_size,\n",
    "              net,\n",
    "              device,\n",
    "              optimizer,\n",
    "              criterion,\n",
    "              label_type,\n",
    "              nclass,\n",
    "              model_uses_batch,\n",
    "              checkpoint_dir,\n",
    "              tensorboard_dir,\n",
    "              num_workers,\n",
    "              use_cuda = True):\n",
    "    \"\"\"\n",
    "        Trains the net nepoch times and saves the model anytime the validation loss decreases\n",
    "    \"\"\"\n",
    "\n",
    "    loader_train = DataLoader(train_data,\n",
    "                            batch_size = train_batch_size,\n",
    "                            shuffle = True,\n",
    "                            num_workers = num_workers,\n",
    "                            drop_last = True,\n",
    "                            pin_memory = False)\n",
    "    loader_valid = DataLoader(valid_data,\n",
    "                            batch_size = valid_batch_size,\n",
    "                            shuffle = True,\n",
    "                            num_workers = 1,\n",
    "                            drop_last = True,\n",
    "                            pin_memory = False)\n",
    "\n",
    "    start_loss = np.inf\n",
    "    writer = SummaryWriter(tensorboard_dir)\n",
    "    for i in range(nepoch):\n",
    "        train_loss, train_met = train_one_epoch(i, net, loader_train, device, optimizer, criterion, label_type, nclass = nclass, model_uses_batch = model_uses_batch)\n",
    "        valid_loss, valid_met = valid_one_epoch(net, loader_valid, device, criterion, label_type, nclass = nclass, model_uses_batch = model_uses_batch)\n",
    "\n",
    "        if valid_loss < start_loss:\n",
    "            save_checkpoint({'state_dict': net.state_dict(),\n",
    "                             'optimizer': optimizer.state_dict()}, f'{checkpoint_dir}/net_checkpoint_{i}.pth.tar')\n",
    "            start_loss = valid_loss\n",
    "\n",
    "        writer.add_scalar('loss/train', train_loss, i)\n",
    "        writer.add_scalar('loss/valid', valid_loss, i)\n",
    "        if label_type == LabelType.Segmentation:\n",
    "            for k, iou in enumerate(train_met):\n",
    "                writer.add_scalar(f'iou/train_{k}class', iou, i)\n",
    "            for k, iou in enumerate(valid_met):\n",
    "                writer.add_scalar(f'iou/valid_{k}class', iou, i)\n",
    "        elif label_type == LabelType.Classification:\n",
    "            writer.add_scalar('acc/train', train_met, i)\n",
    "            writer.add_scalar('acc/valid', valid_met, i)\n",
    "        writer.flush()\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d25d4df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23135cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320356e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8567505",
   "metadata": {},
   "source": [
    "We check for one file that the elaboration of graphs with Networkx and with the NN notation agree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5314574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Small loop to check that my graph creator (the old one) and the new graph representation are the same (more or less)\n",
    "dataset_ = Dataset(file_path, '0nubb').get(1)\n",
    "for i in df.dataset_id.unique():\n",
    "    event = df[df.dataset_id == i]\n",
    "    event = event.reset_index(drop = True)\n",
    "    #need to reset index as the edge index representation works with index from 0 to N (number of nodes in the graph)\n",
    "    G = create_graph(event, np.sqrt(3), ['xbin', 'ybin', 'zbin'])\n",
    "    data = dataset_[i]\n",
    "    n_nodes = data.x.shape[0]\n",
    "    n_edges = data.edge_index.shape[1]\n",
    "    assert n_nodes == len(G.nodes)\n",
    "    assert n_edges == 2 * len(G.edges) \n",
    "    #this factor 2 is due to the fact that the networkx graphs know if a graph is directed or not, but with the tensor representation we have to include both directions\n",
    "    #for each edge to indicate they are not directed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48bee17e",
   "metadata": {},
   "source": [
    "Small function to visualize graph with label colours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6ff0048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_draw(event, color_dct = {1:'b', 2:'y', 3:'r', 4:'b', 5:'y', 6:'r', 7:'g'}):\n",
    "    G = create_graph(event, np.sqrt(3), ['xbin', 'ybin', 'zbin'])\n",
    "    node_colors = event.segclass.apply(lambda x: color_dct[x]).values\n",
    "    nx.draw(G, node_color = node_colors, node_shape = '.')\n",
    "#graph_draw(event)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e22eaea",
   "metadata": {},
   "source": [
    "We also create a function for idx split for a certain dataset (group of processed files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd2d1afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_idx_split(dataset, train_perc):\n",
    "    indices = np.arange(len(dataset))\n",
    "    valid_perc = (1 - train_perc) / 2\n",
    "    random.shuffle(indices)\n",
    "    train_data = torch.tensor(np.sort(indices[:int((len(indices)+1)*train_perc)])) #Remaining 80% to training set\n",
    "    valid_data = torch.tensor(np.sort(indices[int((len(indices)+1)*train_perc):int((len(indices)+1)*(train_perc + valid_perc))]))\n",
    "    test_data = torch.tensor(np.sort(indices[int((len(indices)+1)*(train_perc + valid_perc)):]))\n",
    "    idx_split = {'train':train_data, 'valid':valid_data, 'test':test_data}\n",
    "    return idx_split\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5ccc33f",
   "metadata": {},
   "source": [
    "Example of how the idx split would be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb7a2fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the dataset, index split and data loaders for each case\n",
    "file_path = '/mnt/lustre/scratch/nlsas/home/usc/ie/mpm/NEXT100/labelled_data/0nubb/554mm_voxels/'\n",
    "\n",
    "dataset = Dataset(file_path, '0nubb').join()\n",
    "\n",
    "idx_split = create_idx_split(dataset, 0.8)\n",
    "#idx_split\n",
    "train_loader = DataLoader([dataset[i] for i in idx_split['train']], batch_size=50, shuffle=True, num_workers=0)\n",
    "#valid_loader = DataLoader([dataset[i] for i in idx_split['valid']], batch_size=50, shuffle=False, num_workers=0)\n",
    "#test_loader = DataLoader([dataset[i] for i in idx_split['test']], batch_size=50, shuffle=False, num_workers=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0253c9b8",
   "metadata": {},
   "source": [
    "Implement a function to calculate + correct the weights for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfe0ee5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_loss(file_names, correct = False):\n",
    "    #correct assigns to the ghost class the desired inverse freq and redistributes the rest\n",
    "    seg = pd.Series(dtype='int')\n",
    "    for f in file_names:\n",
    "        seg = seg.append(dio.load_dst(f, 'DATASET', 'BeershebaVoxels').segclass)\n",
    "    freq = np.bincount(seg - 1, minlength=max(seg))\n",
    "    inv_freq = 1. / freq\n",
    "    inv_freq = inv_freq / sum(inv_freq)\n",
    "    if correct:\n",
    "        redistr = inv_freq[:-1] * (1 - correct) / sum(inv_freq[:-1])\n",
    "        inv_freq = np.append(redistr, correct)\n",
    "    return inv_freq\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51a1d335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.37765031, 0.02843595, 0.20542302, 0.23333288, 0.01188655,\n",
       "       0.04327129, 0.1       ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_for_weights = glob(file_path + 'raw/*.h5')\n",
    "inv_freq = weight_loss(files_for_weights, correct = 0.1)\n",
    "inv_freq"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
