{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import copy\n",
    "import random\n",
    "import itertools\n",
    "import numpy   as np\n",
    "import pandas  as pd\n",
    "import os.path as osp\n",
    "from   glob import glob\n",
    "\n",
    "import invisible_cities.io.dst_io as dio\n",
    "\n",
    "import torch\n",
    "from   torch_geometric.data import Data, Dataset\n",
    "from   torch_geometric.data.makedirs import makedirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, root, tag = '0nubb', transform=None, pre_transform=None, pre_filter=None, directed = False, simplify_segclass = False):\n",
    "        self.sort = lambda x: int(x.split('_')[-2])\n",
    "        self.tag = tag\n",
    "        self.directed = directed\n",
    "        self.simplify_segclass = simplify_segclass\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        ''' \n",
    "        Returns a list of the raw files in order (supossing they are beersheba labelled files that have the structure beersheba_label_N_tag.h5)\n",
    "        '''\n",
    "        rfiles = [i.split('/')[-1] for i in glob(self.raw_dir + '/*_{}.h5'.format(self.tag))]\n",
    "        return sorted(rfiles, key = self.sort)\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        '''\n",
    "        Returns a list of the processed files in order (supossing they are stored tensors with the structure data_N.pt)\n",
    "        '''\n",
    "        pfiles = [i.split('/')[-1] for i in glob(self.processed_dir + '/data_*_{}.pt'.format(self.tag))]\n",
    "        return sorted(pfiles, key = self.sort)\n",
    "    \n",
    "    def process(self):\n",
    "        makedirs(self.processed_dir)\n",
    "        already_processed = [self.sort(i) for i in self.processed_file_names]\n",
    "        for raw_path in self.raw_paths:\n",
    "            idx = self.sort(raw_path)\n",
    "            if np.isin(idx, already_processed):\n",
    "                #to avoid processing already processed files\n",
    "                continue\n",
    "            data = graphDataset(raw_path, directed=self.directed, simplify_segclass=self.simplify_segclass)\n",
    "\n",
    "            #if self.pre_filter is not None and not self.pre_filter(data):\n",
    "            #    continue\n",
    "\n",
    "            #if self.pre_transform is not None:\n",
    "            #    data = self.pre_transform(data)\n",
    "\n",
    "            torch.save(data, osp.join(self.processed_dir, f'data_{idx}_{self.tag}.pt'))\n",
    "        \n",
    "\n",
    "    def len(self):\n",
    "        return len(self.processed_file_names)\n",
    "\n",
    "    def get(self, idx):\n",
    "        data = torch.load(osp.join(self.processed_dir, f'data_{idx}_{self.tag}.pt'))\n",
    "        return data\n",
    "\n",
    "    def join(self):\n",
    "        #print('Joining ', self.processed_file_names)\n",
    "        dataset = []\n",
    "        for processed_path in self.processed_paths:\n",
    "            dataset += torch.load(processed_path)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn.models import GraphUNet\n",
    "from torch.nn import BatchNorm1d, CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to create a UNet structure with GCN layers, graph pooling and unpooling, and skip connections between paralell steps of the downsample/upsample. This GraphUNet is already implemented in torch geometric, from the paper https://arxiv.org/abs/1905.05178."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the UNEt they allow me to pass forward the batch tensor so different graphs in the same event are joint by some information now :)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](UNet.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](gPool.png)\n",
    "![Alt text](gUnPool.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(true, pred, **kwrgs):\n",
    "    acc = sum(true == pred) / len(true)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_id, model, loader, device, optimizer, loss_fn, metrics = 'IoU', nclass = 4, model_uses_batch = True):\n",
    "    label_map = {0:0, 1:1, 2:2, 3:0, 4:1, 5:2, 6:3}\n",
    "    # Tell the model it's going to train\n",
    "    model.train()\n",
    "    loss_epoch = 0\n",
    "    if metrics == 'acc':\n",
    "        metric_fn = accuracy\n",
    "        met_epoch = 0\n",
    "    elif metrics == 'IoU':\n",
    "        metric_fn = IoU\n",
    "        met_epoch = np.zeros(nclass)\n",
    "\n",
    "    # Iterate for the batches in the data loader\n",
    "    for batch in loader:\n",
    "        # Pass the batch to device (cuda)\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Zero grad the optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Pass the data to the model\n",
    "        if model_uses_batch:\n",
    "            out = model.forward(batch.x.type(torch.float), batch.edge_index, batch.batch) \n",
    "        else:\n",
    "            out = model.forward(batch.x.type(torch.float), batch.edge_index)\n",
    "\n",
    "        # Now we pass the output and the labels to the loss function\n",
    "        # We will use nll_loss (negative log likelihood, useful to train C classes bc we can add weights for each class)\n",
    "        # This loss will need input (N, C) target (N); being C = num of classes, N = batch size\n",
    "        \n",
    "        # We read the label, transform into long tensor (needed by this loss function), pass to cuda device and shifted by one \n",
    "        # because for the output the classes are from [0, 6] and for the labels they are [1, 7]\n",
    "        label = batch.y.type(torch.LongTensor).to(device) - 1\n",
    "\n",
    "        # The reshape is needed to pass from a (N, 1) shape (automatically appears when doing\n",
    "        # batch.y), to a (N) shape as we need; the output of the net is already (N, C) if it's properly built\n",
    "        loss = loss_fn(out, torch.reshape(label, (-1,)))\n",
    "        \n",
    "        # Back propagation (compute gradients of the loss with respect to the weights in the model)\n",
    "        loss.backward()\n",
    "        # Gradient descent (update the optimizer)\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_epoch += loss.item()\n",
    "\n",
    "\n",
    "        # ####### REVISIT THIS PART, PROBABLY CHANGE LABELS FROM THE BEGGINING WHEN\n",
    "        #Â ####### CREATING THE GRAPHS ALREADY, NOT NOW TO COMPARE!!\n",
    "        #MAYBE adapt metrics function so it can take these things without that much transformation\n",
    "        #This is the out of the net with a shape that is valid as input of the metrics function\n",
    "        pred = torch.reshape(out.argmax(dim=-1, keepdim=True), (-1,)).detach().cpu().numpy()\n",
    "        #This true is the same as label but ready to input the metrics function\n",
    "        true = torch.reshape(batch.y, (-1,)).detach().cpu().numpy() - 1\n",
    "\n",
    "        #Identify the neighbor segclass with their original segclass to compare each node\n",
    "        pred = np.array([label_map[i] for i in pred])\n",
    "        true = np.array([label_map[i] for i in true])\n",
    "\n",
    "        met_epoch += metric_fn(true, pred, nclass = nclass)\n",
    "    \n",
    "    loss_epoch = loss_epoch / len(loader)\n",
    "    #rms\n",
    "    met_epoch  = met_epoch / len(loader)\n",
    "    epoch_ = f\"Train Epoch: {epoch_id}\"\n",
    "    loss_  = f\"\\t Loss: {loss_epoch:.6f}\"\n",
    "    print(epoch_ + loss_)\n",
    "\n",
    "    return loss_epoch, met_epoch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we should have a config with the information:\n",
    "\n",
    "* path: path to the prepared dataset\n",
    "\n",
    "### TRAIN ####\n",
    "* model: model I implemented and want to use (UNet for now, maybe other models after)\n",
    "* optimizer: what i chose to optimize\n",
    "* loss_fn: what i chose to compute loss\n",
    "* metrics: acc if doing classification, IoU if doing segmentation\n",
    "* nclass: will use 3 for MC, and 4 for Beersheba (given that we will take neighbour )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_one_epoch(model, loader, device, loss_fn, metrics = 'IoU', nclass = 4, model_uses_batch = True):\n",
    "    label_map = {0:0, 1:1, 2:2, 3:0, 4:1, 5:2, 6:3}\n",
    "    # Set the model to evaluate\n",
    "    model.eval()\n",
    "\n",
    "    loss_epoch = 0\n",
    "    if metrics == 'acc':\n",
    "        metric_fn = accuracy\n",
    "        met_epoch = 0\n",
    "    elif metrics == 'IoU':\n",
    "        metric_fn = IoU\n",
    "        met_epoch = np.zeros(nclass)\n",
    "\n",
    "    with torch.no_grad():\n",
    "    # Iterate for the batches in the data loader\n",
    "        for batch in loader:\n",
    "            # Put batch into device (cuda)\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            if model_uses_batch:\n",
    "                out = model.forward(batch.x.type(torch.float), batch.edge_index, batch.batch)\n",
    "            else:\n",
    "                out = model.forward(batch.x.type(torch.float), batch.edge_index)\n",
    "        \n",
    "            label = batch.y.type(torch.LongTensor).to(device) - 1\n",
    "\n",
    "            # The reshape is needed to pass from a (N, 1) shape (automatically appears when doing\n",
    "            # batch.y), to a (N) shape as we need; the output of the net is already (N, C) if it's properly built\n",
    "            loss = loss_fn(out, torch.reshape(label, (-1,)))\n",
    "            \n",
    "            loss_epoch += loss.item()\n",
    "\n",
    "            \n",
    "            # For each node set the maximum argument to pick a class\n",
    "            pred = torch.reshape(out.argmax(dim=-1, keepdim=True), (-1,)).detach().cpu().numpy()\n",
    "\n",
    "            #Once again, the labels are shifted by 1 to match the prediction positions (explained in train fun)\n",
    "            true = torch.reshape(batch.y, (-1,)).detach().cpu().numpy() - 1\n",
    "\n",
    "            pred = np.array([label_map[i] for i in pred])\n",
    "            true = np.array([label_map[i] for i in true])\n",
    "            \n",
    "            met_epoch += metric_fn(true, pred, nclass = nclass)\n",
    "            \n",
    "\n",
    "        loss_epoch = loss_epoch / len(loader)\n",
    "        met_epoch  = met_epoch / len(loader)\n",
    "        loss_ = f\"\\t Validation Loss: {loss_epoch:.6f}\"\n",
    "        print(loss_)\n",
    "\n",
    "    return loss_epoch, met_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_idx_split(dataset, train_perc):\n",
    "    indices = np.arange(len(dataset))\n",
    "    valid_perc = (1 - train_perc) / 2\n",
    "    random.shuffle(indices)\n",
    "    train_data = torch.tensor(np.sort(indices[:int((len(indices)+1)*train_perc)])) #Remaining 80% to training set\n",
    "    valid_data = torch.tensor(np.sort(indices[int((len(indices)+1)*train_perc):int((len(indices)+1)*(train_perc + valid_perc))]))\n",
    "    test_data = torch.tensor(np.sort(indices[int((len(indices)+1)*(train_perc + valid_perc)):]))\n",
    "    idx_split = {'train':train_data, 'valid':valid_data, 'test':test_data}\n",
    "    return idx_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments for the net and the train\n",
    "args = {\n",
    "      'device': device,\n",
    "      'nclass':7,\n",
    "      'depth': 4,\n",
    "      'hidden_dim': 20,\n",
    "      'pool_ratio': 0.3,\n",
    "      'lr': 0.001,\n",
    "      'epochs': 100,\n",
    "      'batch_size': 50\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the dataset, index split and data loaders for each case\n",
    "file_path = '/mnt/lustre/scratch/nlsas/home/usc/ie/mpm/NEXT100/labelled_data/0nubb/554mm_voxels/'\n",
    "\n",
    "dataset = Dataset(file_path, '0nubb').join()\n",
    "idx_split = create_idx_split(dataset, 0.8)\n",
    "\n",
    "train_loader = DataLoader([dataset[i] for i in idx_split['train']], batch_size=args['batch_size'], shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader([dataset[i] for i in idx_split['valid']], batch_size=args['batch_size'], shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader([dataset[i] for i in idx_split['test']], batch_size=args['batch_size'], shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_loss(file_names, correct = False):\n",
    "    #correct assigns to the ghost class the desired inverse freq and redistributes the rest\n",
    "    seg = pd.Series(dtype='int')\n",
    "    for f in file_names:\n",
    "        seg = seg.append(dio.load_dst(f, 'DATASET', 'BeershebaVoxels').segclass)\n",
    "    freq = np.bincount(seg - 1, minlength=max(seg))\n",
    "    inv_freq = 1. / freq\n",
    "    inv_freq = inv_freq / sum(inv_freq)\n",
    "    if correct:\n",
    "        redistr = inv_freq[:-1] * (1 - correct) / sum(inv_freq[:-1])\n",
    "        inv_freq = np.append(redistr, correct)\n",
    "    return inv_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_for_weights = glob(file_path + 'raw/*.h5')\n",
    "inv_freq = weight_loss(files_for_weights, correct = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the model with the previous args and set to device\n",
    "#Activation is relu\n",
    "model = GraphUNet(dataset[0].num_features, args['hidden_dim'],\n",
    "                args['nclass'], args['depth'],\n",
    "                args['pool_ratio']).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set true if we want to train in the next cell\n",
    "start_train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 01, Loss: 1.9408, Train: 4.63%, Valid: 3.84% Test: 4.00%\n",
      "Blob IoU train: 0.00%, Blob IoU valid: 0.00%, Blob IoU test: 0.00%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 02, Loss: 1.9336, Train: 4.63%, Valid: 3.84% Test: 4.00%\n",
      "Blob IoU train: 0.00%, Blob IoU valid: 0.00%, Blob IoU test: 0.00%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 03, Loss: 1.9270, Train: 4.63%, Valid: 3.84% Test: 4.00%\n",
      "Blob IoU train: 0.00%, Blob IoU valid: 0.00%, Blob IoU test: 0.00%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 04, Loss: 1.9179, Train: 4.63%, Valid: 3.84% Test: 4.00%\n",
      "Blob IoU train: 0.01%, Blob IoU valid: 0.00%, Blob IoU test: 0.00%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 05, Loss: 1.9080, Train: 4.96%, Valid: 4.25% Test: 4.26%\n",
      "Blob IoU train: 2.01%, Blob IoU valid: 2.37%, Blob IoU test: 1.63%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 06, Loss: 1.9047, Train: 11.26%, Valid: 10.15% Test: 10.36%\n",
      "Blob IoU train: 24.35%, Blob IoU valid: 21.82%, Blob IoU test: 23.46%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 07, Loss: 1.8899, Train: 15.82%, Valid: 14.96% Test: 14.83%\n",
      "Blob IoU train: 28.62%, Blob IoU valid: 27.77%, Blob IoU test: 28.05%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 08, Loss: 1.8746, Train: 17.48%, Valid: 16.50% Test: 16.81%\n",
      "Blob IoU train: 28.08%, Blob IoU valid: 27.26%, Blob IoU test: 28.28%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 09, Loss: 1.8718, Train: 18.03%, Valid: 17.08% Test: 17.59%\n",
      "Blob IoU train: 27.05%, Blob IoU valid: 26.34%, Blob IoU test: 27.72%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 10, Loss: 1.8593, Train: 18.22%, Valid: 17.18% Test: 17.94%\n",
      "Blob IoU train: 26.38%, Blob IoU valid: 25.83%, Blob IoU test: 27.24%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 11, Loss: 1.8497, Train: 22.95%, Valid: 22.04% Test: 23.01%\n",
      "Blob IoU train: 25.94%, Blob IoU valid: 25.47%, Blob IoU test: 26.93%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 12, Loss: 1.8293, Train: 45.65%, Valid: 46.27% Test: 48.05%\n",
      "Blob IoU train: 25.67%, Blob IoU valid: 25.16%, Blob IoU test: 26.62%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 13, Loss: 1.8167, Train: 46.08%, Valid: 46.68% Test: 48.59%\n",
      "Blob IoU train: 25.70%, Blob IoU valid: 25.19%, Blob IoU test: 26.64%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 14, Loss: 1.7982, Train: 48.47%, Valid: 49.35% Test: 51.05%\n",
      "Blob IoU train: 25.78%, Blob IoU valid: 25.30%, Blob IoU test: 26.72%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 15, Loss: 1.7972, Train: 50.27%, Valid: 50.96% Test: 52.71%\n",
      "Blob IoU train: 25.93%, Blob IoU valid: 25.49%, Blob IoU test: 26.86%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 16, Loss: 1.7961, Train: 51.66%, Valid: 52.30% Test: 53.98%\n",
      "Blob IoU train: 26.34%, Blob IoU valid: 25.91%, Blob IoU test: 27.26%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 17, Loss: 1.7884, Train: 53.15%, Valid: 53.54% Test: 55.34%\n",
      "Blob IoU train: 26.87%, Blob IoU valid: 26.23%, Blob IoU test: 27.56%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 18, Loss: 1.7997, Train: 54.78%, Valid: 55.03% Test: 56.55%\n",
      "Blob IoU train: 27.24%, Blob IoU valid: 26.49%, Blob IoU test: 27.69%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 19, Loss: 1.7989, Train: 55.98%, Valid: 56.24% Test: 57.33%\n",
      "Blob IoU train: 27.66%, Blob IoU valid: 26.92%, Blob IoU test: 27.88%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 20, Loss: 1.7529, Train: 56.45%, Valid: 56.93% Test: 57.90%\n",
      "Blob IoU train: 27.85%, Blob IoU valid: 27.28%, Blob IoU test: 28.23%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 21, Loss: 1.7337, Train: 56.46%, Valid: 56.67% Test: 57.71%\n",
      "Blob IoU train: 28.19%, Blob IoU valid: 27.10%, Blob IoU test: 28.31%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 22, Loss: 1.7523, Train: 56.52%, Valid: 56.86% Test: 58.01%\n",
      "Blob IoU train: 28.21%, Blob IoU valid: 27.02%, Blob IoU test: 28.51%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 23, Loss: 1.7636, Train: 57.10%, Valid: 57.42% Test: 58.44%\n",
      "Blob IoU train: 28.41%, Blob IoU valid: 26.91%, Blob IoU test: 28.49%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 24, Loss: 1.7435, Train: 57.41%, Valid: 57.57% Test: 58.80%\n",
      "Blob IoU train: 28.44%, Blob IoU valid: 26.99%, Blob IoU test: 28.57%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 25, Loss: 1.7448, Train: 57.00%, Valid: 57.24% Test: 58.52%\n",
      "Blob IoU train: 28.46%, Blob IoU valid: 27.09%, Blob IoU test: 28.80%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 26, Loss: 1.7354, Train: 58.65%, Valid: 58.72% Test: 59.93%\n",
      "Blob IoU train: 28.50%, Blob IoU valid: 27.05%, Blob IoU test: 28.86%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 27, Loss: 1.7449, Train: 58.87%, Valid: 58.99% Test: 60.11%\n",
      "Blob IoU train: 28.50%, Blob IoU valid: 26.62%, Blob IoU test: 28.99%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 28, Loss: 1.7264, Train: 58.13%, Valid: 58.25% Test: 59.36%\n",
      "Blob IoU train: 28.49%, Blob IoU valid: 26.60%, Blob IoU test: 28.85%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 29, Loss: 1.7573, Train: 56.42%, Valid: 56.35% Test: 57.82%\n",
      "Blob IoU train: 28.50%, Blob IoU valid: 26.48%, Blob IoU test: 28.81%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 30, Loss: 1.7191, Train: 57.35%, Valid: 57.23% Test: 58.68%\n",
      "Blob IoU train: 28.50%, Blob IoU valid: 26.42%, Blob IoU test: 28.91%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 31, Loss: 1.7321, Train: 58.66%, Valid: 58.56% Test: 59.76%\n",
      "Blob IoU train: 28.44%, Blob IoU valid: 26.30%, Blob IoU test: 28.82%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 32, Loss: 1.7235, Train: 58.44%, Valid: 58.40% Test: 59.64%\n",
      "Blob IoU train: 28.42%, Blob IoU valid: 26.26%, Blob IoU test: 28.72%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 33, Loss: 1.7045, Train: 57.13%, Valid: 57.20% Test: 58.49%\n",
      "Blob IoU train: 28.36%, Blob IoU valid: 26.37%, Blob IoU test: 28.72%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 34, Loss: 1.7283, Train: 57.74%, Valid: 57.78% Test: 59.02%\n",
      "Blob IoU train: 28.36%, Blob IoU valid: 26.29%, Blob IoU test: 28.77%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 35, Loss: 1.7007, Train: 58.96%, Valid: 59.11% Test: 60.05%\n",
      "Blob IoU train: 28.27%, Blob IoU valid: 26.30%, Blob IoU test: 28.71%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 36, Loss: 1.7590, Train: 59.09%, Valid: 59.24% Test: 60.17%\n",
      "Blob IoU train: 28.30%, Blob IoU valid: 26.39%, Blob IoU test: 28.79%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 37, Loss: 1.7442, Train: 57.46%, Valid: 57.67% Test: 58.66%\n",
      "Blob IoU train: 28.19%, Blob IoU valid: 26.30%, Blob IoU test: 28.42%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 38, Loss: 1.7256, Train: 57.81%, Valid: 58.16% Test: 58.92%\n",
      "Blob IoU train: 28.25%, Blob IoU valid: 26.26%, Blob IoU test: 28.43%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 39, Loss: 1.7510, Train: 57.83%, Valid: 58.26% Test: 58.96%\n",
      "Blob IoU train: 28.24%, Blob IoU valid: 26.37%, Blob IoU test: 28.39%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 40, Loss: 1.7171, Train: 57.47%, Valid: 57.90% Test: 58.78%\n",
      "Blob IoU train: 28.13%, Blob IoU valid: 26.09%, Blob IoU test: 28.46%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 41, Loss: 1.7187, Train: 57.98%, Valid: 58.40% Test: 59.21%\n",
      "Blob IoU train: 28.19%, Blob IoU valid: 26.28%, Blob IoU test: 28.58%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 42, Loss: 1.7242, Train: 58.78%, Valid: 59.02% Test: 59.90%\n",
      "Blob IoU train: 28.19%, Blob IoU valid: 26.20%, Blob IoU test: 28.43%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 43, Loss: 1.6909, Train: 59.33%, Valid: 59.52% Test: 60.51%\n",
      "Blob IoU train: 28.10%, Blob IoU valid: 26.03%, Blob IoU test: 28.39%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 44, Loss: 1.7228, Train: 58.16%, Valid: 58.58% Test: 59.31%\n",
      "Blob IoU train: 28.09%, Blob IoU valid: 26.05%, Blob IoU test: 28.40%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 45, Loss: 1.7107, Train: 58.50%, Valid: 58.89% Test: 59.54%\n",
      "Blob IoU train: 28.05%, Blob IoU valid: 26.02%, Blob IoU test: 28.13%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 46, Loss: 1.7062, Train: 58.71%, Valid: 59.02% Test: 59.74%\n",
      "Blob IoU train: 28.07%, Blob IoU valid: 26.04%, Blob IoU test: 28.30%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 47, Loss: 1.6935, Train: 59.18%, Valid: 59.47% Test: 60.16%\n",
      "Blob IoU train: 28.00%, Blob IoU valid: 26.02%, Blob IoU test: 28.19%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 48, Loss: 1.7482, Train: 60.26%, Valid: 60.27% Test: 61.28%\n",
      "Blob IoU train: 28.10%, Blob IoU valid: 26.02%, Blob IoU test: 28.51%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 49, Loss: 1.6842, Train: 59.08%, Valid: 59.21% Test: 60.18%\n",
      "Blob IoU train: 27.67%, Blob IoU valid: 25.71%, Blob IoU test: 27.80%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 50, Loss: 1.7034, Train: 57.68%, Valid: 58.15% Test: 58.73%\n",
      "Blob IoU train: 27.99%, Blob IoU valid: 26.03%, Blob IoU test: 28.06%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 51, Loss: 1.6992, Train: 58.31%, Valid: 58.54% Test: 59.44%\n",
      "Blob IoU train: 27.56%, Blob IoU valid: 25.51%, Blob IoU test: 27.82%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 52, Loss: 1.6813, Train: 59.09%, Valid: 59.38% Test: 60.24%\n",
      "Blob IoU train: 27.80%, Blob IoU valid: 25.93%, Blob IoU test: 28.02%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 53, Loss: 1.7464, Train: 61.09%, Valid: 61.14% Test: 62.03%\n",
      "Blob IoU train: 28.01%, Blob IoU valid: 25.99%, Blob IoU test: 28.19%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 54, Loss: 1.6971, Train: 58.17%, Valid: 58.49% Test: 59.34%\n",
      "Blob IoU train: 27.61%, Blob IoU valid: 25.57%, Blob IoU test: 27.86%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 55, Loss: 1.6978, Train: 58.24%, Valid: 58.56% Test: 59.40%\n",
      "Blob IoU train: 27.64%, Blob IoU valid: 25.73%, Blob IoU test: 28.00%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 56, Loss: 1.6729, Train: 58.66%, Valid: 58.77% Test: 59.84%\n",
      "Blob IoU train: 27.58%, Blob IoU valid: 25.49%, Blob IoU test: 27.77%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 57, Loss: 1.7414, Train: 59.76%, Valid: 59.81% Test: 60.79%\n",
      "Blob IoU train: 27.83%, Blob IoU valid: 25.83%, Blob IoU test: 28.06%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 58, Loss: 1.7048, Train: 59.03%, Valid: 59.12% Test: 60.20%\n",
      "Blob IoU train: 27.52%, Blob IoU valid: 25.50%, Blob IoU test: 27.71%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 59, Loss: 1.6804, Train: 58.52%, Valid: 58.71% Test: 59.67%\n",
      "Blob IoU train: 27.77%, Blob IoU valid: 25.81%, Blob IoU test: 28.07%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 60, Loss: 1.7427, Train: 59.62%, Valid: 59.70% Test: 60.53%\n",
      "Blob IoU train: 27.82%, Blob IoU valid: 25.85%, Blob IoU test: 28.01%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 61, Loss: 1.7154, Train: 60.92%, Valid: 60.97% Test: 61.95%\n",
      "Blob IoU train: 27.58%, Blob IoU valid: 25.58%, Blob IoU test: 27.83%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 62, Loss: 1.7064, Train: 60.56%, Valid: 60.61% Test: 61.58%\n",
      "Blob IoU train: 27.77%, Blob IoU valid: 25.84%, Blob IoU test: 28.05%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 63, Loss: 1.6746, Train: 58.44%, Valid: 58.60% Test: 59.61%\n",
      "Blob IoU train: 27.42%, Blob IoU valid: 25.43%, Blob IoU test: 27.66%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 64, Loss: 1.6833, Train: 57.54%, Valid: 57.85% Test: 58.69%\n",
      "Blob IoU train: 27.43%, Blob IoU valid: 25.44%, Blob IoU test: 27.52%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 65, Loss: 1.7026, Train: 59.68%, Valid: 59.77% Test: 60.54%\n",
      "Blob IoU train: 27.74%, Blob IoU valid: 25.70%, Blob IoU test: 27.84%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 66, Loss: 1.6710, Train: 59.66%, Valid: 59.78% Test: 60.87%\n",
      "Blob IoU train: 27.32%, Blob IoU valid: 25.44%, Blob IoU test: 27.59%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 67, Loss: 1.6535, Train: 59.88%, Valid: 59.92% Test: 60.85%\n",
      "Blob IoU train: 27.81%, Blob IoU valid: 25.84%, Blob IoU test: 27.95%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 68, Loss: 1.6936, Train: 58.98%, Valid: 59.10% Test: 60.13%\n",
      "Blob IoU train: 27.52%, Blob IoU valid: 25.62%, Blob IoU test: 27.78%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 69, Loss: 1.6727, Train: 59.87%, Valid: 60.05% Test: 60.94%\n",
      "Blob IoU train: 27.71%, Blob IoU valid: 25.87%, Blob IoU test: 27.81%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 70, Loss: 1.6910, Train: 60.27%, Valid: 60.32% Test: 61.35%\n",
      "Blob IoU train: 27.75%, Blob IoU valid: 25.77%, Blob IoU test: 27.86%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 71, Loss: 1.6732, Train: 59.02%, Valid: 59.03% Test: 59.95%\n",
      "Blob IoU train: 27.49%, Blob IoU valid: 25.43%, Blob IoU test: 27.60%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 72, Loss: 1.7100, Train: 59.88%, Valid: 59.91% Test: 60.96%\n",
      "Blob IoU train: 27.87%, Blob IoU valid: 25.78%, Blob IoU test: 28.12%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 73, Loss: 1.7080, Train: 59.81%, Valid: 59.86% Test: 60.87%\n",
      "Blob IoU train: 27.42%, Blob IoU valid: 25.40%, Blob IoU test: 27.52%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 74, Loss: 1.7306, Train: 59.51%, Valid: 59.59% Test: 60.55%\n",
      "Blob IoU train: 27.50%, Blob IoU valid: 25.55%, Blob IoU test: 27.73%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 75, Loss: 1.7097, Train: 59.13%, Valid: 59.28% Test: 60.13%\n",
      "Blob IoU train: 27.68%, Blob IoU valid: 25.66%, Blob IoU test: 27.79%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 76, Loss: 1.7155, Train: 59.04%, Valid: 59.22% Test: 60.05%\n",
      "Blob IoU train: 27.63%, Blob IoU valid: 25.62%, Blob IoU test: 27.75%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 77, Loss: 1.6721, Train: 58.18%, Valid: 58.35% Test: 59.23%\n",
      "Blob IoU train: 27.35%, Blob IoU valid: 25.32%, Blob IoU test: 27.68%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 78, Loss: 1.7106, Train: 58.83%, Valid: 58.99% Test: 59.90%\n",
      "Blob IoU train: 27.51%, Blob IoU valid: 25.60%, Blob IoU test: 27.76%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 79, Loss: 1.7033, Train: 59.12%, Valid: 59.28% Test: 60.13%\n",
      "Blob IoU train: 27.47%, Blob IoU valid: 25.62%, Blob IoU test: 27.99%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 80, Loss: 1.6844, Train: 58.77%, Valid: 58.92% Test: 59.64%\n",
      "Blob IoU train: 27.64%, Blob IoU valid: 25.68%, Blob IoU test: 27.60%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 81, Loss: 1.6739, Train: 60.83%, Valid: 60.63% Test: 61.66%\n",
      "Blob IoU train: 27.79%, Blob IoU valid: 25.68%, Blob IoU test: 27.69%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 82, Loss: 1.7573, Train: 60.90%, Valid: 60.80% Test: 61.80%\n",
      "Blob IoU train: 27.85%, Blob IoU valid: 25.88%, Blob IoU test: 27.99%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 83, Loss: 1.7460, Train: 57.77%, Valid: 57.98% Test: 58.79%\n",
      "Blob IoU train: 27.17%, Blob IoU valid: 25.02%, Blob IoU test: 27.55%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 84, Loss: 1.6949, Train: 57.04%, Valid: 57.33% Test: 58.18%\n",
      "Blob IoU train: 27.62%, Blob IoU valid: 25.59%, Blob IoU test: 27.87%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 85, Loss: 1.7178, Train: 60.93%, Valid: 60.91% Test: 61.65%\n",
      "Blob IoU train: 28.19%, Blob IoU valid: 26.13%, Blob IoU test: 28.22%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 86, Loss: 1.6847, Train: 59.35%, Valid: 59.40% Test: 60.38%\n",
      "Blob IoU train: 27.39%, Blob IoU valid: 25.31%, Blob IoU test: 27.92%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 87, Loss: 1.6999, Train: 58.88%, Valid: 58.90% Test: 59.87%\n",
      "Blob IoU train: 27.34%, Blob IoU valid: 25.14%, Blob IoU test: 27.77%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 88, Loss: 1.7025, Train: 59.52%, Valid: 59.67% Test: 60.41%\n",
      "Blob IoU train: 28.09%, Blob IoU valid: 26.05%, Blob IoU test: 28.20%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 89, Loss: 1.6806, Train: 60.85%, Valid: 60.66% Test: 61.69%\n",
      "Blob IoU train: 27.90%, Blob IoU valid: 25.77%, Blob IoU test: 28.06%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 90, Loss: 1.6685, Train: 58.95%, Valid: 58.94% Test: 59.92%\n",
      "Blob IoU train: 27.30%, Blob IoU valid: 25.06%, Blob IoU test: 27.80%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 91, Loss: 1.7544, Train: 59.01%, Valid: 59.05% Test: 59.82%\n",
      "Blob IoU train: 27.86%, Blob IoU valid: 25.81%, Blob IoU test: 27.93%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 92, Loss: 1.7150, Train: 58.40%, Valid: 58.48% Test: 59.39%\n",
      "Blob IoU train: 27.04%, Blob IoU valid: 24.83%, Blob IoU test: 27.38%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 93, Loss: 1.6685, Train: 57.88%, Valid: 58.07% Test: 58.92%\n",
      "Blob IoU train: 27.80%, Blob IoU valid: 25.67%, Blob IoU test: 27.96%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 94, Loss: 1.7402, Train: 58.54%, Valid: 58.53% Test: 59.50%\n",
      "Blob IoU train: 27.70%, Blob IoU valid: 25.61%, Blob IoU test: 28.01%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 95, Loss: 1.6403, Train: 60.66%, Valid: 60.54% Test: 61.53%\n",
      "Blob IoU train: 27.80%, Blob IoU valid: 25.74%, Blob IoU test: 28.20%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 96, Loss: 1.7073, Train: 60.63%, Valid: 60.63% Test: 61.47%\n",
      "Blob IoU train: 28.24%, Blob IoU valid: 26.12%, Blob IoU test: 28.34%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 97, Loss: 1.7097, Train: 59.11%, Valid: 59.23% Test: 60.02%\n",
      "Blob IoU train: 27.62%, Blob IoU valid: 25.61%, Blob IoU test: 27.90%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 98, Loss: 1.7177, Train: 59.51%, Valid: 59.58% Test: 60.35%\n",
      "Blob IoU train: 27.83%, Blob IoU valid: 25.77%, Blob IoU test: 28.09%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 99, Loss: 1.6809, Train: 60.10%, Valid: 60.17% Test: 60.84%\n",
      "Blob IoU train: 28.04%, Blob IoU valid: 25.90%, Blob IoU test: 28.05%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 100, Loss: 1.7417, Train: 59.16%, Valid: 59.30% Test: 60.08%\n",
      "Blob IoU train: 27.33%, Blob IoU valid: 25.37%, Blob IoU test: 27.64%, \n"
     ]
    }
   ],
   "source": [
    "if start_train:\n",
    "  # Start from zero the model (not using a trained model)\n",
    "  model.reset_parameters()\n",
    "\n",
    "  # Initiate the optimizer with the model parameters and a learning rate\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
    "\n",
    "  # Pick the loss function\n",
    "  loss_fn = CrossEntropyLoss(weight=torch.Tensor(inv_freq).to(device)) #torch.nn.NLLLoss(weight=torch.Tensor(inv_freq).to(device)) #\n",
    "\n",
    "  best_model = None\n",
    "  best_valid_acc = 0\n",
    "\n",
    "  # Iterate on the number of epochs\n",
    "  pred = []\n",
    "  true = []\n",
    "  for epoch in range(1, 1 + args[\"epochs\"]):\n",
    "    # Train the model with the fucntion\n",
    "    print('Training...')\n",
    "    loss = train(model, train_loader, device, optimizer, loss_fn)\n",
    "\n",
    "    #Evaluate the model with the function and for the 3 sets of data\n",
    "    print('Evaluating...')\n",
    "    train_acc, train_iou, _, _ = eval(model, train_loader, device)\n",
    "    val_acc, val_iou, y_pred, y_true = eval(model, valid_loader, device)\n",
    "    test_acc, test_iou, _, _ = eval(model, test_loader, device)\n",
    "    #pred.append(y_pred)\n",
    "    #true.append(y_true)\n",
    "    # Store the model if the validation accuracy improved\n",
    "    if val_acc > best_valid_acc:\n",
    "        best_valid_acc = val_acc\n",
    "        best_model = copy.deepcopy(model)\n",
    "    # Print the important variables for epoch\n",
    "    print(f'Epoch: {epoch:02d}, '\n",
    "          f'Loss: {loss:.4f}, '\n",
    "          f'Train: {100 * train_acc:.2f}%, '\n",
    "          f'Valid: {100 * val_acc:.2f}% '\n",
    "          f'Test: {100 * test_acc:.2f}%')\n",
    "    print(f'Blob IoU train: {100 * train_iou[-2]:.2f}%, '\n",
    "          f'Blob IoU valid: {100 * val_iou[-2]:.2f}%, '\n",
    "          f'Blob IoU test: {100 * test_iou[-2]:.2f}%, ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: Train: 61.09%, Valid: 61.14% Test: 62.03%\n",
      "Best model IoU blob: Train: 28.01%, Valid: 25.99% Test: 28.19%\n"
     ]
    }
   ],
   "source": [
    "train_acc, train_iou, _, _ = eval(best_model, train_loader, device)\n",
    "valid_acc, valid_iou, _, _ = eval(best_model, valid_loader, device)\n",
    "test_acc, test_iou, _, _  = eval(best_model, test_loader, device)\n",
    "\n",
    "print(f'Best model: '\n",
    "      f'Train: {100 * train_acc:.2f}%, '\n",
    "      f'Valid: {100 * valid_acc:.2f}% '\n",
    "      f'Test: {100 * test_acc:.2f}%')\n",
    "print(f'Best model IoU blob: '\n",
    "      f'Train: {100 * train_iou[-2]:.2f}%, '\n",
    "      f'Valid: {100 * valid_iou[-2]:.2f}% '\n",
    "      f'Test: {100 * test_iou[-2]:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
