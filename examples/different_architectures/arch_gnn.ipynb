{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GENERAL STEPS FOR NN FUNCTIONING:\n",
    "* Load a net model and send to device. Load their weights if we have an already trained model.\n",
    "* TRAIN:\n",
    "    * Choose loss function with their parameters. Load/compute weights if needed.\n",
    "    * Choose optimizer and its parameters.\n",
    "    * Load the data to train.\n",
    "    * Iterate per epoch and train:\n",
    "        * Set net to train and choose the metrics (goodness indicators)\n",
    "        * Iterate per batch in the loader, send batch to device, zero grad the optimizer, forward the data through the net, evaluate loss, back propagation for the loss and gradient descent for the optimizer (update one step).\n",
    "        * Save per epoch the loss and the metrics (I'm not doing the latter in this notebook)\n",
    "    * Save net configuration if validation loss has descended.\n",
    "* Evaluate: repeat the process with evaluation of the model, and a softmax to decide the dominant class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import copy\n",
    "import random\n",
    "import itertools\n",
    "import numpy   as np\n",
    "import pandas  as pd\n",
    "import os.path as osp\n",
    "from   glob import glob\n",
    "\n",
    "import invisible_cities.io.dst_io as dio\n",
    "\n",
    "import torch\n",
    "from   torch_geometric.data import Data, Dataset\n",
    "from   torch_geometric.data.makedirs import makedirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_index(event, max_distance = np.sqrt(3), coord_names = ['xbin', 'ybin', 'zbin'], directed = False):\n",
    "    ''' \n",
    "    Creates the edge index tensor, with shape [2, E] where E is the number of edges.\n",
    "    It contains the index of the nodes that are connected by an edge. \n",
    "    '''\n",
    "    coord = event[coord_names].T\n",
    "    edges = []\n",
    "    if directed: node_comb = itertools.combinations(coord, 2)\n",
    "    else: node_comb = itertools.permutations(coord, 2)\n",
    "    for i, j in node_comb:\n",
    "        dis = np.linalg.norm(coord[i].values - coord[j].values)\n",
    "        if dis <= max_distance:\n",
    "            edges.append([i, j])\n",
    "    edges = torch.tensor(edges).T\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphData(event, features = ['energy'], label_n = ['segclass'], max_distance = np.sqrt(3), coord_names = ['xbin', 'ybin', 'zbin'], directed = False, simplify_segclass = False):\n",
    "    edges = edge_index(event, max_distance=max_distance, coord_names=coord_names, directed=directed)\n",
    "    #nodes features, for now just the energy; the node itself is defined by its position\n",
    "    nodes = torch.tensor(event[features].values)\n",
    "    #nodes segmentation label\n",
    "    seg = event[label_n].values\n",
    "    if simplify_segclass:\n",
    "        label_map = {1:1, 2:2, 3:3, 4:1, 5:2, 6:3, 7:4}\n",
    "        seg = np.array([label_map[i] for i in seg])\n",
    "    #we can try to add also the transformation just to have track + blob (+ ghost)\n",
    "    label = torch.tensor(seg)\n",
    "    #maybe add binclass label?\n",
    "    graph_data = Data(edge_index = edges, x = nodes, y = label, num_nodes = len(nodes))\n",
    "    return graph_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphDataset(file, features = ['energy'], label_n = ['segclass'], max_distance = np.sqrt(3), coord_names = ['xbin', 'ybin', 'zbin'], directed = False, simplify_segclass = False):\n",
    "    df = dio.load_dst(file, 'DATASET', 'BeershebaVoxels')\n",
    "    dataset = []\n",
    "    for dat_id, event in df.groupby('dataset_id'):\n",
    "        event = event.reset_index(drop = True)\n",
    "        graph_data = graphData(event, features=features, label_n=label_n, max_distance=max_distance, coord_names=coord_names, directed = directed, simplify_segclass = simplify_segclass)\n",
    "        dataset.append(graph_data)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, root, tag = '0nubb', transform=None, pre_transform=None, pre_filter=None, directed = False, simplify_segclass = False):\n",
    "        self.sort = lambda x: int(x.split('_')[-2])\n",
    "        self.tag = tag\n",
    "        self.directed = directed\n",
    "        self.simplify_segclass = simplify_segclass\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        ''' \n",
    "        Returns a list of the raw files in order (supossing they are beersheba labelled files that have the structure beersheba_label_N_tag.h5)\n",
    "        '''\n",
    "        rfiles = [i.split('/')[-1] for i in glob(self.raw_dir + '/*_{}.h5'.format(self.tag))]\n",
    "        return sorted(rfiles, key = self.sort)\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        '''\n",
    "        Returns a list of the processed files in order (supossing they are stored tensors with the structure data_N.pt)\n",
    "        '''\n",
    "        pfiles = [i.split('/')[-1] for i in glob(self.processed_dir + '/data_*_{}.pt'.format(self.tag))]\n",
    "        return sorted(pfiles, key = self.sort)\n",
    "    \n",
    "    def process(self):\n",
    "        makedirs(self.processed_dir)\n",
    "        already_processed = [self.sort(i) for i in self.processed_file_names]\n",
    "        for raw_path in self.raw_paths:\n",
    "            idx = self.sort(raw_path)\n",
    "            if np.isin(idx, already_processed):\n",
    "                #to avoid processing already processed files\n",
    "                continue\n",
    "            data = graphDataset(raw_path, directed=self.directed, simplify_segclass=self.simplify_segclass)\n",
    "\n",
    "            #if self.pre_filter is not None and not self.pre_filter(data):\n",
    "            #    continue\n",
    "\n",
    "            #if self.pre_transform is not None:\n",
    "            #    data = self.pre_transform(data)\n",
    "\n",
    "            torch.save(data, osp.join(self.processed_dir, f'data_{idx}_{self.tag}.pt'))\n",
    "        \n",
    "\n",
    "    def len(self):\n",
    "        return len(self.processed_file_names)\n",
    "\n",
    "    def get(self, idx):\n",
    "        data = torch.load(osp.join(self.processed_dir, f'data_{idx}_{self.tag}.pt'))\n",
    "        return data\n",
    "\n",
    "    def join(self):\n",
    "        #print('Joining ', self.processed_file_names)\n",
    "        dataset = []\n",
    "        for processed_path in self.processed_paths:\n",
    "            dataset += torch.load(processed_path)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import BatchNorm1d, CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure of the mock GNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](mock_gcn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers,\n",
    "                 dropout, return_embeds=False):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        # A list of GCNConv layers\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "\n",
    "        # A list of 1D batch normalization layers\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "\n",
    "        # The log softmax layer\n",
    "        self.softmax = None\n",
    "\n",
    "        ############ BUILDING THE LAYERS ##############\n",
    "        ## 1. Following the image, there are GCNConv num_layers and BatchNorm1d num_layers - 1\n",
    "        ## 2. The GCNConv will have input_dim as the num of node features (1 for now, the energy)\n",
    "        ## 3. The hidden_dim is the dimension in the inner channels, in here it's kept constant but\n",
    "        ##    in a future could be changed during the process\n",
    "        ## 4. The output_dim should be equal to the number of classes we can have per node\n",
    "        ## 5. The output dim of a GCNConv is the input dim for the following\n",
    "\n",
    "        self.convs.append(GCNConv(input_dim, hidden_dim))\n",
    "        self.bns.append(BatchNorm1d(hidden_dim))\n",
    "        for i in range(num_layers - 1): #take one because the first layer was already written outside\n",
    "          self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "          self.bns.append(BatchNorm1d(hidden_dim))\n",
    "        self.convs.append(GCNConv(hidden_dim, output_dim))\n",
    "        #no bns append here as the last GCNConv is followed by the softmax\n",
    "\n",
    "        #softmax is applied in the 1 dimension (to each node individually)\n",
    "        self.softmax = torch.nn.LogSoftmax(dim = 1)\n",
    "\n",
    "        # Probability of an element getting zeroed\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        # Applies the rectified linear unit function (keeps only positive values)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "        #########################################\n",
    "\n",
    "        # Skip classification layer and return node embeddings\n",
    "        self.return_embeds = return_embeds\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        out = None\n",
    "\n",
    "        ############# PASS THE VECTORS ############\n",
    "        ## 1. We need now to pass the vectors through the layers\n",
    "        ## 2. If return_embeds is True, then skip the last softmax layer\n",
    "        for conv, bn in zip(self.convs, self.bns):\n",
    "          x = conv(x, edge_index)\n",
    "          x = bn(x)\n",
    "          x = self.relu(self.dropout(x))\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        if self.return_embeds: out = x\n",
    "        else: out = self.softmax(x)\n",
    "        #########################################\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, device, optimizer, loss_fn):\n",
    "    # Tell the model it's going to train\n",
    "    model.train()\n",
    "    loss = 0\n",
    "\n",
    "    # Iterate for the batches in the data loader\n",
    "    for batch in loader:\n",
    "        # Pass the batch to device (cuda)\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Zero grad the optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Pass the data to the model\n",
    "        out = model.forward(batch.x.type(torch.float), batch.edge_index) \n",
    "\n",
    "        # Now we pass the output and the labels to the loss function\n",
    "        # We will use nll_loss (negative log likelihood, useful to train C classes bc we can add weights for each class)\n",
    "        # This loss will need input (N, C) target (N); being C = num of classes, N = batch size\n",
    "        \n",
    "        # We read the label, transform into long tensor (needed by this loss function), pass to cuda device and shifted by one \n",
    "        # because for the output the classes are from [0, 6] and for the labels they are [1, 7]\n",
    "        label = batch.y.type(torch.LongTensor).to(device) - 1\n",
    "\n",
    "        # The reshape is needed to pass from a (N, 1) shape (automatically appears when doing\n",
    "        # batch.y), to a (N) shape as we need; the output of the net is already (N, C) if it's properly built\n",
    "        loss = loss_fn(out, torch.reshape(label, (-1,)))\n",
    "        \n",
    "        # Back propagation (compute gradients of the loss with respect to the weights in the model)\n",
    "        loss.backward()\n",
    "        # Gradient descent (update the optimizer)\n",
    "        optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, true):\n",
    "    acc = sum(pred == true) / len(pred)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IoU(pred, true, nclass = 3):\n",
    "    \"\"\"\n",
    "        Intersection over union is a metric for semantic segmentation.\n",
    "        It returns a IoU value for each class of our input tensors/arrays.\n",
    "    \"\"\"\n",
    "    eps = sys.float_info.epsilon\n",
    "    confusion_matrix = np.zeros((nclass, nclass))\n",
    "\n",
    "    for i in range(len(true)):\n",
    "        confusion_matrix[true[i]][pred[i]] += 1\n",
    "\n",
    "    IoU = []\n",
    "    for i in range(nclass):\n",
    "        IoU.append((confusion_matrix[i, i] + eps) / (sum(confusion_matrix[:, i]) + sum(confusion_matrix[i, :]) - confusion_matrix[i, i] + eps))\n",
    "    return np.array(IoU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, loader, device):\n",
    "    # Set the model to evaluate\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    # Iterate for the batches in the data loader\n",
    "    for batch in loader:\n",
    "        # Put batch into device (cuda)\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Pass the data to the model\n",
    "        with torch.no_grad():\n",
    "            out = model.forward(batch.x.type(torch.float), batch.edge_index)\n",
    "        \n",
    "        # For each node set the maximum argument to pick a class\n",
    "        pred = out.argmax(dim=-1, keepdim=True)  \n",
    "\n",
    "        #Once again, the labels are shifted by 1 to match the prediction positions (explained in train fun)\n",
    "        true = torch.reshape(batch.y, (-1,)).detach().cpu() - 1\n",
    "        \n",
    "        #Append the results to lists\n",
    "        y_pred.append(torch.reshape(pred, (-1,)).detach().cpu())\n",
    "        y_true.append(true)\n",
    "    \n",
    "    #Concatenate the items in the list and transform into array\n",
    "    y_pred = torch.cat(y_pred).numpy()\n",
    "    y_true = torch.cat(y_true).numpy()\n",
    "\n",
    "    #Identify the neighbor segclass with their original segclass to compare each node\n",
    "    label_map = {0:0, 1:1, 2:2, 3:0, 4:1, 5:2, 6:3}\n",
    "    y_pred = np.array([label_map[i] for i in y_pred])\n",
    "    y_true = np.array([label_map[i] for i in y_true])\n",
    "    \n",
    "    # Compare and return an accuracy (number of success nodes / all nodes)\n",
    "    # Not the best, it is better the IoU for segmentation\n",
    "    acc = accuracy(y_pred, y_true)\n",
    "    iou = IoU(y_pred, y_true, nclass=4)\n",
    "    return acc, iou, y_pred, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_idx_split(dataset, train_perc):\n",
    "    indices = np.arange(len(dataset))\n",
    "    valid_perc = (1 - train_perc) / 2\n",
    "    random.shuffle(indices)\n",
    "    train_data = torch.tensor(np.sort(indices[:int((len(indices)+1)*train_perc)])) #Remaining 80% to training set\n",
    "    valid_data = torch.tensor(np.sort(indices[int((len(indices)+1)*train_perc):int((len(indices)+1)*(train_perc + valid_perc))]))\n",
    "    test_data = torch.tensor(np.sort(indices[int((len(indices)+1)*(train_perc + valid_perc)):]))\n",
    "    idx_split = {'train':train_data, 'valid':valid_data, 'test':test_data}\n",
    "    return idx_split\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device: {}'.format(device))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments for the net and the train\n",
    "args = {\n",
    "      'device': device,\n",
    "      'nclass':7,\n",
    "      'num_layers': 3,\n",
    "      'hidden_dim': 30,\n",
    "      'dropout': 0.3,\n",
    "      'lr': 0.001,\n",
    "      'epochs': 100,\n",
    "      'batch_size': 50\n",
    "  }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the train/validation/test sets of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the dataset, index split and data loaders for each case\n",
    "file_path = '/mnt/lustre/scratch/nlsas/home/usc/ie/mpm/NEXT100/labelled_data/0nubb/554mm_voxels/'\n",
    "\n",
    "dataset = Dataset(file_path, '0nubb').join()\n",
    "idx_split = create_idx_split(dataset, 0.8)\n",
    "\n",
    "train_loader = DataLoader([dataset[i] for i in idx_split['train']], batch_size=args['batch_size'], shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader([dataset[i] for i in idx_split['valid']], batch_size=args['batch_size'], shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader([dataset[i] for i in idx_split['test']], batch_size=args['batch_size'], shuffle=False, num_workers=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the weight loss per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_loss(file_names, correct = False):\n",
    "    #correct assigns to the ghost class the desired inverse freq and redistributes the rest\n",
    "    seg = pd.Series(dtype='int')\n",
    "    for f in file_names:\n",
    "        seg = seg.append(dio.load_dst(f, 'DATASET', 'BeershebaVoxels').segclass)\n",
    "    freq = np.bincount(seg - 1, minlength=max(seg))\n",
    "    inv_freq = 1. / freq\n",
    "    inv_freq = inv_freq / sum(inv_freq)\n",
    "    if correct:\n",
    "        redistr = inv_freq[:-1] * (1 - correct) / sum(inv_freq[:-1])\n",
    "        inv_freq = np.append(redistr, correct)\n",
    "    return inv_freq\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_for_weights = glob(file_path + 'raw/*.h5')\n",
    "inv_freq = weight_loss(files_for_weights, correct = 0.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the model with the previous args and set to device\n",
    "model = GCN(dataset[0].num_features, args['hidden_dim'],\n",
    "              args['nclass'], args['num_layers'],\n",
    "              args['dropout']).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set true if we want to train in the next cell\n",
    "start_train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 01, Loss: 2.1279, Train: 77.36%, Valid: 76.66% Test: 77.22%\n",
      "Blob IoU train: 0.00%, Blob IoU valid: 0.00%, Blob IoU test: 0.00%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 02, Loss: 2.0201, Train: 77.36%, Valid: 76.66% Test: 77.22%\n",
      "Blob IoU train: 0.00%, Blob IoU valid: 0.00%, Blob IoU test: 0.00%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 03, Loss: 1.9473, Train: 77.36%, Valid: 76.66% Test: 77.22%\n",
      "Blob IoU train: 0.00%, Blob IoU valid: 0.00%, Blob IoU test: 0.00%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 04, Loss: 1.9292, Train: 8.28%, Valid: 7.52% Test: 9.20%\n",
      "Blob IoU train: 0.00%, Blob IoU valid: 0.00%, Blob IoU test: 0.00%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 05, Loss: 1.8599, Train: 4.38%, Valid: 4.14% Test: 5.65%\n",
      "Blob IoU train: 0.00%, Blob IoU valid: 0.00%, Blob IoU test: 0.00%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 06, Loss: 1.8659, Train: 4.38%, Valid: 4.14% Test: 5.65%\n",
      "Blob IoU train: 0.00%, Blob IoU valid: 0.00%, Blob IoU test: 0.00%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 07, Loss: 1.8185, Train: 4.38%, Valid: 4.14% Test: 5.65%\n",
      "Blob IoU train: 0.00%, Blob IoU valid: 0.00%, Blob IoU test: 0.00%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 08, Loss: 1.8372, Train: 4.38%, Valid: 4.14% Test: 5.65%\n",
      "Blob IoU train: 0.00%, Blob IoU valid: 0.00%, Blob IoU test: 0.00%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 09, Loss: 1.8209, Train: 4.38%, Valid: 4.14% Test: 5.65%\n",
      "Blob IoU train: 0.00%, Blob IoU valid: 0.00%, Blob IoU test: 0.00%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 10, Loss: 1.8216, Train: 4.46%, Valid: 4.25% Test: 5.65%\n",
      "Blob IoU train: 0.43%, Blob IoU valid: 0.60%, Blob IoU test: 0.00%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 11, Loss: 1.7993, Train: 5.61%, Valid: 4.84% Test: 6.13%\n",
      "Blob IoU train: 4.07%, Blob IoU valid: 3.65%, Blob IoU test: 2.84%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 12, Loss: 1.7994, Train: 6.06%, Valid: 5.24% Test: 6.68%\n",
      "Blob IoU train: 4.03%, Blob IoU valid: 3.30%, Blob IoU test: 4.32%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 13, Loss: 1.7205, Train: 13.98%, Valid: 14.05% Test: 15.36%\n",
      "Blob IoU train: 4.98%, Blob IoU valid: 4.18%, Blob IoU test: 4.32%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 14, Loss: 1.7564, Train: 30.60%, Valid: 31.01% Test: 30.21%\n",
      "Blob IoU train: 3.65%, Blob IoU valid: 3.50%, Blob IoU test: 4.84%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 15, Loss: 1.7634, Train: 33.13%, Valid: 35.24% Test: 34.94%\n",
      "Blob IoU train: 28.35%, Blob IoU valid: 31.69%, Blob IoU test: 29.37%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 16, Loss: 1.7275, Train: 47.59%, Valid: 48.93% Test: 47.98%\n",
      "Blob IoU train: 29.11%, Blob IoU valid: 34.48%, Blob IoU test: 32.14%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 17, Loss: 1.7558, Train: 68.80%, Valid: 73.21% Test: 69.25%\n",
      "Blob IoU train: 27.42%, Blob IoU valid: 33.09%, Blob IoU test: 29.09%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 18, Loss: 1.7431, Train: 77.08%, Valid: 77.72% Test: 76.36%\n",
      "Blob IoU train: 6.44%, Blob IoU valid: 9.81%, Blob IoU test: 4.81%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 19, Loss: 1.7882, Train: 50.90%, Valid: 52.34% Test: 50.80%\n",
      "Blob IoU train: 20.01%, Blob IoU valid: 26.85%, Blob IoU test: 17.14%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 20, Loss: 1.7684, Train: 73.73%, Valid: 77.48% Test: 72.70%\n",
      "Blob IoU train: 23.67%, Blob IoU valid: 29.20%, Blob IoU test: 21.97%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 21, Loss: 1.7481, Train: 70.05%, Valid: 73.99% Test: 71.15%\n",
      "Blob IoU train: 28.52%, Blob IoU valid: 33.56%, Blob IoU test: 31.05%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 22, Loss: 1.7767, Train: 37.38%, Valid: 40.13% Test: 38.97%\n",
      "Blob IoU train: 28.29%, Blob IoU valid: 33.23%, Blob IoU test: 31.15%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 23, Loss: 1.7451, Train: 14.68%, Valid: 16.12% Test: 16.78%\n",
      "Blob IoU train: 28.74%, Blob IoU valid: 33.94%, Blob IoU test: 31.54%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 24, Loss: 1.7190, Train: 14.70%, Valid: 16.09% Test: 16.70%\n",
      "Blob IoU train: 28.95%, Blob IoU valid: 34.11%, Blob IoU test: 31.62%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 25, Loss: 1.7385, Train: 15.72%, Valid: 17.08% Test: 17.84%\n",
      "Blob IoU train: 28.53%, Blob IoU valid: 33.02%, Blob IoU test: 30.70%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 26, Loss: 1.7843, Train: 16.70%, Valid: 17.77% Test: 18.05%\n",
      "Blob IoU train: 27.45%, Blob IoU valid: 30.26%, Blob IoU test: 27.64%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 27, Loss: 1.7478, Train: 21.83%, Valid: 22.53% Test: 23.94%\n",
      "Blob IoU train: 27.02%, Blob IoU valid: 29.38%, Blob IoU test: 26.65%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 28, Loss: 1.7436, Train: 13.79%, Valid: 15.24% Test: 15.87%\n",
      "Blob IoU train: 27.83%, Blob IoU valid: 33.73%, Blob IoU test: 30.89%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 29, Loss: 1.7628, Train: 12.70%, Valid: 13.82% Test: 14.66%\n",
      "Blob IoU train: 27.43%, Blob IoU valid: 33.36%, Blob IoU test: 30.64%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 30, Loss: 1.7573, Train: 14.41%, Valid: 15.86% Test: 16.25%\n",
      "Blob IoU train: 29.02%, Blob IoU valid: 34.47%, Blob IoU test: 31.80%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 31, Loss: 1.7834, Train: 12.54%, Valid: 13.41% Test: 14.25%\n",
      "Blob IoU train: 28.15%, Blob IoU valid: 33.75%, Blob IoU test: 30.52%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 32, Loss: 1.7190, Train: 13.40%, Valid: 14.50% Test: 15.22%\n",
      "Blob IoU train: 28.69%, Blob IoU valid: 34.26%, Blob IoU test: 32.02%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 33, Loss: 1.6847, Train: 8.66%, Valid: 9.02% Test: 9.80%\n",
      "Blob IoU train: 19.55%, Blob IoU valid: 23.39%, Blob IoU test: 19.96%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 34, Loss: 1.7035, Train: 6.68%, Valid: 7.05% Test: 7.46%\n",
      "Blob IoU train: 11.90%, Blob IoU valid: 15.94%, Blob IoU test: 10.67%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 35, Loss: 1.6965, Train: 4.80%, Valid: 4.43% Test: 5.75%\n",
      "Blob IoU train: 2.40%, Blob IoU valid: 2.14%, Blob IoU test: 1.08%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 36, Loss: 1.7183, Train: 4.68%, Valid: 4.29% Test: 5.68%\n",
      "Blob IoU train: 1.67%, Blob IoU valid: 1.18%, Blob IoU test: 0.36%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 37, Loss: 1.7146, Train: 4.68%, Valid: 4.25% Test: 5.85%\n",
      "Blob IoU train: 1.69%, Blob IoU valid: 1.02%, Blob IoU test: 1.27%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 38, Loss: 1.6903, Train: 5.82%, Valid: 4.97% Test: 6.82%\n",
      "Blob IoU train: 7.93%, Blob IoU valid: 5.15%, Blob IoU test: 7.38%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 39, Loss: 1.7157, Train: 15.18%, Valid: 16.40% Test: 16.97%\n",
      "Blob IoU train: 26.71%, Blob IoU valid: 34.00%, Blob IoU test: 27.97%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 40, Loss: 1.7160, Train: 31.34%, Valid: 33.67% Test: 30.10%\n",
      "Blob IoU train: 24.04%, Blob IoU valid: 30.92%, Blob IoU test: 23.58%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 41, Loss: 1.7205, Train: 46.56%, Valid: 50.68% Test: 44.48%\n",
      "Blob IoU train: 22.14%, Blob IoU valid: 29.64%, Blob IoU test: 22.08%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 42, Loss: 1.7557, Train: 51.91%, Valid: 56.23% Test: 50.17%\n",
      "Blob IoU train: 28.34%, Blob IoU valid: 34.33%, Blob IoU test: 30.99%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 43, Loss: 1.7176, Train: 40.06%, Valid: 40.49% Test: 36.37%\n",
      "Blob IoU train: 27.76%, Blob IoU valid: 31.26%, Blob IoU test: 28.70%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 44, Loss: 1.7054, Train: 16.46%, Valid: 17.43% Test: 18.24%\n",
      "Blob IoU train: 27.89%, Blob IoU valid: 30.89%, Blob IoU test: 28.78%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 45, Loss: 1.7671, Train: 15.41%, Valid: 16.82% Test: 17.78%\n",
      "Blob IoU train: 29.02%, Blob IoU valid: 33.96%, Blob IoU test: 32.18%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 46, Loss: 1.7235, Train: 42.08%, Valid: 42.74% Test: 38.70%\n",
      "Blob IoU train: 26.21%, Blob IoU valid: 28.52%, Blob IoU test: 26.26%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 47, Loss: 1.7077, Train: 56.52%, Valid: 60.05% Test: 56.55%\n",
      "Blob IoU train: 28.32%, Blob IoU valid: 32.80%, Blob IoU test: 30.45%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 48, Loss: 1.7197, Train: 55.09%, Valid: 58.19% Test: 55.05%\n",
      "Blob IoU train: 28.10%, Blob IoU valid: 32.91%, Blob IoU test: 30.53%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 49, Loss: 1.6892, Train: 51.19%, Valid: 53.00% Test: 49.55%\n",
      "Blob IoU train: 28.20%, Blob IoU valid: 34.04%, Blob IoU test: 31.14%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 50, Loss: 1.7455, Train: 54.68%, Valid: 57.17% Test: 53.64%\n",
      "Blob IoU train: 29.05%, Blob IoU valid: 34.10%, Blob IoU test: 32.18%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 51, Loss: 1.7439, Train: 18.80%, Valid: 19.74% Test: 19.87%\n",
      "Blob IoU train: 26.42%, Blob IoU valid: 32.93%, Blob IoU test: 28.82%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 52, Loss: 1.7113, Train: 38.40%, Valid: 40.20% Test: 36.05%\n",
      "Blob IoU train: 28.75%, Blob IoU valid: 34.19%, Blob IoU test: 31.77%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 53, Loss: 1.6922, Train: 44.26%, Valid: 46.68% Test: 42.03%\n",
      "Blob IoU train: 28.69%, Blob IoU valid: 33.40%, Blob IoU test: 31.59%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 54, Loss: 1.6776, Train: 44.42%, Valid: 46.65% Test: 41.70%\n",
      "Blob IoU train: 27.98%, Blob IoU valid: 32.92%, Blob IoU test: 30.47%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 55, Loss: 1.7022, Train: 42.35%, Valid: 43.58% Test: 39.31%\n",
      "Blob IoU train: 28.59%, Blob IoU valid: 33.02%, Blob IoU test: 31.19%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 56, Loss: 1.7185, Train: 43.25%, Valid: 44.87% Test: 40.36%\n",
      "Blob IoU train: 28.05%, Blob IoU valid: 32.89%, Blob IoU test: 30.61%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 57, Loss: 1.6907, Train: 53.03%, Valid: 56.60% Test: 53.18%\n",
      "Blob IoU train: 25.75%, Blob IoU valid: 32.41%, Blob IoU test: 27.67%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 58, Loss: 1.7260, Train: 52.76%, Valid: 55.63% Test: 51.99%\n",
      "Blob IoU train: 27.53%, Blob IoU valid: 33.37%, Blob IoU test: 29.87%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 59, Loss: 1.7407, Train: 42.98%, Valid: 46.15% Test: 43.73%\n",
      "Blob IoU train: 28.03%, Blob IoU valid: 32.58%, Blob IoU test: 29.95%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 60, Loss: 1.7465, Train: 29.97%, Valid: 32.85% Test: 32.90%\n",
      "Blob IoU train: 27.38%, Blob IoU valid: 30.42%, Blob IoU test: 28.10%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 61, Loss: 1.7006, Train: 17.66%, Valid: 19.07% Test: 18.62%\n",
      "Blob IoU train: 26.27%, Blob IoU valid: 28.97%, Blob IoU test: 26.31%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 62, Loss: 1.7306, Train: 27.00%, Valid: 28.74% Test: 26.45%\n",
      "Blob IoU train: 27.69%, Blob IoU valid: 31.45%, Blob IoU test: 28.96%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 63, Loss: 1.7061, Train: 28.90%, Valid: 30.19% Test: 27.66%\n",
      "Blob IoU train: 27.84%, Blob IoU valid: 31.33%, Blob IoU test: 28.42%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 64, Loss: 1.6921, Train: 20.24%, Valid: 21.35% Test: 21.72%\n",
      "Blob IoU train: 28.76%, Blob IoU valid: 33.34%, Blob IoU test: 31.95%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 65, Loss: 1.7095, Train: 13.45%, Valid: 15.20% Test: 15.36%\n",
      "Blob IoU train: 27.19%, Blob IoU valid: 33.49%, Blob IoU test: 29.12%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 66, Loss: 1.6930, Train: 12.42%, Valid: 13.59% Test: 14.51%\n",
      "Blob IoU train: 23.15%, Blob IoU valid: 27.69%, Blob IoU test: 24.61%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 67, Loss: 1.7377, Train: 14.75%, Valid: 16.26% Test: 14.74%\n",
      "Blob IoU train: 25.08%, Blob IoU valid: 32.07%, Blob IoU test: 24.80%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 68, Loss: 1.7191, Train: 21.58%, Valid: 22.32% Test: 20.96%\n",
      "Blob IoU train: 25.76%, Blob IoU valid: 29.14%, Blob IoU test: 26.37%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 69, Loss: 1.7420, Train: 15.82%, Valid: 17.70% Test: 17.43%\n",
      "Blob IoU train: 27.09%, Blob IoU valid: 30.87%, Blob IoU test: 28.62%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 70, Loss: 1.7036, Train: 16.10%, Valid: 17.52% Test: 17.99%\n",
      "Blob IoU train: 27.82%, Blob IoU valid: 31.49%, Blob IoU test: 29.44%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 71, Loss: 1.7035, Train: 25.49%, Valid: 28.01% Test: 26.45%\n",
      "Blob IoU train: 27.90%, Blob IoU valid: 31.05%, Blob IoU test: 28.66%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 72, Loss: 1.7222, Train: 16.61%, Valid: 17.62% Test: 17.73%\n",
      "Blob IoU train: 26.77%, Blob IoU valid: 29.34%, Blob IoU test: 26.89%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 73, Loss: 1.7388, Train: 31.35%, Valid: 33.07% Test: 32.55%\n",
      "Blob IoU train: 28.06%, Blob IoU valid: 32.27%, Blob IoU test: 30.62%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 74, Loss: 1.7177, Train: 18.66%, Valid: 20.97% Test: 21.49%\n",
      "Blob IoU train: 26.23%, Blob IoU valid: 31.38%, Blob IoU test: 29.35%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 75, Loss: 1.6892, Train: 13.04%, Valid: 14.42% Test: 15.04%\n",
      "Blob IoU train: 25.49%, Blob IoU valid: 31.07%, Blob IoU test: 27.52%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 76, Loss: 1.6814, Train: 8.72%, Valid: 9.65% Test: 9.40%\n",
      "Blob IoU train: 19.23%, Blob IoU valid: 25.92%, Blob IoU test: 18.05%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 77, Loss: 1.7129, Train: 8.20%, Valid: 8.83% Test: 8.66%\n",
      "Blob IoU train: 17.71%, Blob IoU valid: 23.61%, Blob IoU test: 15.74%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 78, Loss: 1.7511, Train: 11.24%, Valid: 12.50% Test: 11.94%\n",
      "Blob IoU train: 23.80%, Blob IoU valid: 31.25%, Blob IoU test: 21.47%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 79, Loss: 1.7110, Train: 18.14%, Valid: 20.06% Test: 20.14%\n",
      "Blob IoU train: 18.89%, Blob IoU valid: 26.05%, Blob IoU test: 16.27%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 80, Loss: 1.7134, Train: 23.80%, Valid: 26.12% Test: 24.66%\n",
      "Blob IoU train: 19.63%, Blob IoU valid: 26.58%, Blob IoU test: 16.95%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 81, Loss: 1.6823, Train: 21.73%, Valid: 24.50% Test: 21.04%\n",
      "Blob IoU train: 25.24%, Blob IoU valid: 32.20%, Blob IoU test: 25.71%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 82, Loss: 1.7022, Train: 53.88%, Valid: 58.67% Test: 53.24%\n",
      "Blob IoU train: 28.44%, Blob IoU valid: 34.52%, Blob IoU test: 31.34%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 83, Loss: 1.7533, Train: 67.57%, Valid: 71.71% Test: 69.09%\n",
      "Blob IoU train: 28.05%, Blob IoU valid: 33.54%, Blob IoU test: 30.72%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 84, Loss: 1.6947, Train: 12.33%, Valid: 13.65% Test: 14.42%\n",
      "Blob IoU train: 25.38%, Blob IoU valid: 31.37%, Blob IoU test: 28.03%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 85, Loss: 1.7277, Train: 38.40%, Valid: 39.87% Test: 37.15%\n",
      "Blob IoU train: 27.81%, Blob IoU valid: 31.40%, Blob IoU test: 28.87%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 86, Loss: 1.6863, Train: 17.34%, Valid: 18.45% Test: 18.11%\n",
      "Blob IoU train: 24.43%, Blob IoU valid: 26.16%, Blob IoU test: 24.35%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 87, Loss: 1.6842, Train: 17.54%, Valid: 18.59% Test: 17.85%\n",
      "Blob IoU train: 23.89%, Blob IoU valid: 25.65%, Blob IoU test: 23.44%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 88, Loss: 1.7090, Train: 17.81%, Valid: 18.93% Test: 16.95%\n",
      "Blob IoU train: 19.20%, Blob IoU valid: 20.49%, Blob IoU test: 18.31%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 89, Loss: 1.7118, Train: 17.89%, Valid: 19.22% Test: 17.18%\n",
      "Blob IoU train: 20.22%, Blob IoU valid: 21.93%, Blob IoU test: 19.58%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 90, Loss: 1.7023, Train: 43.95%, Valid: 46.27% Test: 43.29%\n",
      "Blob IoU train: 28.13%, Blob IoU valid: 32.56%, Blob IoU test: 30.55%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 91, Loss: 1.6952, Train: 52.91%, Valid: 56.25% Test: 50.92%\n",
      "Blob IoU train: 27.33%, Blob IoU valid: 33.06%, Blob IoU test: 28.94%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 92, Loss: 1.6445, Train: 30.84%, Valid: 32.01% Test: 30.25%\n",
      "Blob IoU train: 26.91%, Blob IoU valid: 29.35%, Blob IoU test: 26.83%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 93, Loss: 1.7185, Train: 53.29%, Valid: 56.45% Test: 52.23%\n",
      "Blob IoU train: 27.58%, Blob IoU valid: 32.63%, Blob IoU test: 29.92%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 94, Loss: 1.6643, Train: 7.62%, Valid: 8.13% Test: 8.33%\n",
      "Blob IoU train: 15.67%, Blob IoU valid: 20.76%, Blob IoU test: 14.45%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 95, Loss: 1.7017, Train: 8.41%, Valid: 9.25% Test: 8.79%\n",
      "Blob IoU train: 18.26%, Blob IoU valid: 25.06%, Blob IoU test: 16.11%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 96, Loss: 1.7236, Train: 16.45%, Valid: 17.66% Test: 16.17%\n",
      "Blob IoU train: 22.82%, Blob IoU valid: 30.58%, Blob IoU test: 21.65%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 97, Loss: 1.6908, Train: 21.12%, Valid: 21.86% Test: 21.77%\n",
      "Blob IoU train: 15.21%, Blob IoU valid: 21.93%, Blob IoU test: 13.79%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 98, Loss: 1.7323, Train: 23.77%, Valid: 22.21% Test: 24.29%\n",
      "Blob IoU train: 8.97%, Blob IoU valid: 11.69%, Blob IoU test: 7.05%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 99, Loss: 1.6903, Train: 15.68%, Valid: 14.24% Test: 16.94%\n",
      "Blob IoU train: 9.58%, Blob IoU valid: 12.44%, Blob IoU test: 7.69%, \n",
      "Training...\n",
      "Evaluating...\n",
      "Epoch: 100, Loss: 1.6739, Train: 12.10%, Valid: 13.53% Test: 13.55%\n",
      "Blob IoU train: 26.15%, Blob IoU valid: 32.89%, Blob IoU test: 27.09%, \n"
     ]
    }
   ],
   "source": [
    "if start_train:\n",
    "  # Start from zero the model (not using a trained model)\n",
    "  model.reset_parameters()\n",
    "\n",
    "  # Initiate the optimizer with the model parameters and a learning rate\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
    "\n",
    "  # Pick the loss function\n",
    "  loss_fn = torch.nn.NLLLoss(weight=torch.Tensor(inv_freq).to(device)) #CrossEntropyLoss(weight=torch.Tensor(inv_freq).to(device)) #\n",
    "\n",
    "  best_model = None\n",
    "  best_valid_acc = 0\n",
    "\n",
    "  # Iterate on the number of epochs\n",
    "  pred = []\n",
    "  true = []\n",
    "  for epoch in range(1, 1 + args[\"epochs\"]):\n",
    "    # Train the model with the fucntion\n",
    "    print('Training...')\n",
    "    loss = train(model, train_loader, device, optimizer, loss_fn)\n",
    "\n",
    "    #Evaluate the model with the function and for the 3 sets of data\n",
    "    print('Evaluating...')\n",
    "    train_acc, train_iou, _, _ = eval(model, train_loader, device)\n",
    "    val_acc, val_iou, y_pred, y_true = eval(model, valid_loader, device)\n",
    "    test_acc, test_iou, _, _ = eval(model, test_loader, device)\n",
    "    #pred.append(y_pred)\n",
    "    #true.append(y_true)\n",
    "    # Store the model if the validation accuracy improved\n",
    "    if val_acc > best_valid_acc:\n",
    "        best_valid_acc = val_acc\n",
    "        best_model = copy.deepcopy(model)\n",
    "    # Print the important variables for epoch\n",
    "    print(f'Epoch: {epoch:02d}, '\n",
    "          f'Loss: {loss:.4f}, '\n",
    "          f'Train: {100 * train_acc:.2f}%, '\n",
    "          f'Valid: {100 * val_acc:.2f}% '\n",
    "          f'Test: {100 * test_acc:.2f}%')\n",
    "    print(f'Blob IoU train: {100 * train_iou[-2]:.2f}%, '\n",
    "          f'Blob IoU valid: {100 * val_iou[-2]:.2f}%, '\n",
    "          f'Blob IoU test: {100 * test_iou[-2]:.2f}%, ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: Train: 77.08%, Valid: 77.72% Test: 76.36%\n",
      "Best model IoU blob: Train: 6.44%, Valid: 9.81% Test: 4.81%\n"
     ]
    }
   ],
   "source": [
    "train_acc, train_iou, _, _ = eval(best_model, train_loader, device)\n",
    "valid_acc, valid_iou, _, _ = eval(best_model, valid_loader, device)\n",
    "test_acc, test_iou, _, _  = eval(best_model, test_loader, device)\n",
    "\n",
    "print(f'Best model: '\n",
    "      f'Train: {100 * train_acc:.2f}%, '\n",
    "      f'Valid: {100 * valid_acc:.2f}% '\n",
    "      f'Test: {100 * test_acc:.2f}%')\n",
    "print(f'Best model IoU blob: '\n",
    "      f'Train: {100 * train_iou[-2]:.2f}%, '\n",
    "      f'Valid: {100 * valid_iou[-2]:.2f}% '\n",
    "      f'Test: {100 * test_iou[-2]:.2f}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It could work better, but I guess there are several factors to take into account for which this is failing:\n",
    "* DISCONNECTED TRACKS! We still have this problem, maybe we can try bigger voxelization if this works\n",
    "* Maybe TOO COMPLEX GRAPHS (lots of nodes and edges) without much information... Bigger voxels or clouds JA analysis could help with that\n",
    "* NOT CLEAN DATA, lots of ghosts etc that can be easily cleaned (although with the disconnected tracks becomes more dangerous)\n",
    "* NOT EVENT INFORMATION; I'm passing graphs to the net without any information of the event they are in. For this, it should take into account the batch information. Search information on that, to see if there are any layers for the graphs that involve this info.\n",
    "* NET ARCHITECTURE: not the optimal, since there are things that maybe don't match with what we need (for example the previous bullet)\n",
    "* NET PARAMETERS: there are some params that we could play with\n",
    "* LOSS FUNCTION: not used yet the weights, and don't know either if this is the optimal one; could try focal loss or something as we used with the regular segmentation\n",
    "* TRAIN MORE THOUGHTFULLY; not all the data is being used...\n",
    "* BATCHING; Play also with the batching, maybe batching 1 by 1 makes the net aware of each individual event...\n",
    "* 1 TRACK CUT WOULD SOLVE EVENT-GRAPH RELATION; for now it's not compatible witht the track disconnection.\n",
    "* TRY CLASSIFICATION NET; need to change this a bit and think how to unify the thing graph - event, or how to label each graph instead of each event...\n",
    "* TRY SIMPLIFYING THE INPUT CLASSES; instead of reducing them after passing the DNN, process data to already have it simplified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, y_pred, y_true = eval(best_model, valid_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 302, 8018,  955]), array([0., 1., 2., 3.]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.histogram(y_pred, max(np.unique(y_pred)) + 1, range = (min(np.unique(y_pred)), max(np.unique(y_pred)) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 311, 7290, 1671,    3]), array([0., 1., 2., 3., 4.]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.histogram(y_true, max(np.unique(y_true)) + 1, range = (min(np.unique(y_true)), max(np.unique(y_true)) + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unbalance between other and track class... Blob class seems to be right!! So maybe we must try no differences between track and other...\n",
    "\n",
    "I hope this method is only adding and deciding by the sum of the energy etc... Should try other architectures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
